# 个人学习笔记

## 1.HDFS

### 1.1 架构：

- Master（NameNode/NN）， 带 N 个 Slaves（DataNode/DN）
- 一个存储文件会被拆分成多个Block
  - blocksize：128M（hadoop 2.x版本默认）
  - 130M ==》 2个 Block 128M 和 2M
- NN：
  - 负责客户端请求的相应；
  - 负责存储元数据（文件的名称、副本系数、Block存放的DN）的管理
- DN
  - 存储用户的文件对应的数据块（Block）
  - 定期向NN发送心跳信息，汇报自身以及所有的Block信息，健康状况。
- replication factor：副本系数、副本因子。

### 1.2 搭建Hadoop集群环境(三节点)

> **前置工作：**

- 准备三台虚拟机，这里使用VMWare创建三台Centos7的虚拟机；

| hostname | ip            |
| -------- | ------------- |
| node1    | 192.168.3.100 |
| node2    | 192.168.3.101 |
| node3    | 192.168.3.102 |

- 配置静态ip；
- 关闭防火墙；
- 关闭selinux安全级别，`/etc/selinux/config`中的`SELINUX=disabled`；
- 设置三台虚拟机的域名和IP的配置文件，`vi /etc/hosts`；
- 设置三台虚拟机免密登录-生成密钥以及分发密钥；
- 安装JDK；
- 时间同步：

```shell
#node1虚拟机作为时间主节点，下载ntp

yum install ntp -y

#修改配置文件，vi /etc/ntp.conf

restrict 192.168.254.0 mask 255.255.255.0 nomodify notrap

server 127.127.1.0(自己作为主节点，注释掉server开头的四行)

#启动ntp，并设置为开机启动

systemctl start ntpd

systemctl enable ntpd

#node2和node3作为从节点，下载ntpdate并同步数据

yum install ntpdate -y

ntpdate -u node1

#为了防止时间不一致，做定时器，定时同步node1时间数据,输出信息到空设备文件

crontab -e * * * * * /usr/sbin/ntpdate -u node1 >/dev/null 2>&1
```

- 安装Hadoop。

> **集群搭建：**

- 进到/usr/local/hadoop/etc/hadoop，修改hdfs-site.xml文件，在configuration标签里添加以下内容;

```xml
<!-- core-site.xml -->
<configuration>
    <!-- 在hdfs的地址名称:schame, ip, port -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://node1:8020</value>
    </property>
    <!-- hdfs的一个基础路径，被其他属性所依赖的一个基础路径 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/root/app/hadoop/tmp</value>
    </property>
</configuration>
```

```xml
<!-- hdfs-site.xml -->
<configuration>
    <!-- namenode守护进程管理的元数据文件fsimage存储的位置-->
    <property>
    <name>dfs.namenode.name.dir</name>
    <value>file://${hadoop.tmp.dir}/dfs/name</value>
    </property>
    <!-- 确定DFS数据节点应该将其块存储在本地文件系统的何处-->
    <property>
    <name>dfs.datanode.data.dir</name>
    <value>file://${hadoop.tmp.dir}/dfs/data</value>
    </property>
    <!-- 块的副本数-->
    <property>
    <name>dfs.replication</name>
        <value>3</value>
    </property>
    <!-- 块的大小(128M)，下面的单位是字节-->
    <property>
        <name>dfs.blocksize</name>
        <value>134217728</value>
    </property>
    <!-- secondarynamenode守护进程的http地址:主机名和端口号。参考守护进程布局-->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>node2:50090</value>
    </property>
    <!-- namenode守护进程的http地址:主机名和端口号。参考守护进程布局-->
    <property>
        <name>dfs.namenode.http-address</name>
        <value>node1:50070 </value>
    </property>
</configuration>
```

- 在mapred-site.xml文件configuration标签里添加以下内容-这里不存在mapred-site.xml文件，需要将mapred-site.xml.template文件复制后重命名；

```xml
<!-- mapred-site.xml -->
<configuration>
    <!-- 指定mapreduce使用yarn资源管理器-->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property><!-- 配置作业历史服务器的地址-->
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>node1:10020</value></property>
    <!--配置作业历史服务器的http地址-->
    <property>
        <name>mapreduce.jobhistory.webappaddress</name>
        <value>node1:19888</value>
    </property>
</configuration>
```

- 在yarn-site.xml文件configuration标签里添加以下内容；

```xml
<!-- yarn-site.xml -->
<configuration>
    <!-- 指定yarn的shuffle技术-->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <!-- 指定resourcemanager的主机名-->
    <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>node1</value>
    </property>
    <!--下面的可选-->
    <!--指定shuffle对应的类 -->
    <property>
    <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <!--配置resourcemanager的内部通讯地址-->
    <property>
        <name>yarn.resourcemanager.address</name>
        <value>node1:8032</value>
    </property>
    <!--配置resourcemanager的scheduler的内部通讯地址-->
    <property>
        <name>yarn,resourcemanager.scheduler.address</name>
        <value>node1:8030</value>
    </property>
    <!--配置resoucemanager的资源调度的内部通讯地址-->
    <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>node1:8031</value>
    </property>
    <!--配置resourcemanager的管理员的内部通讯地址-->
    <property>
        <name>yarn.resourcemanager.admin.address</name>
        <value>node1:8033</value>
    </property>
    <!--配置resourcemanager的web ui 的监控页面-->
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>node1:8088</value>
    </property>
</configuration>
```

- 在hadoop-env.sh文件修改JDK安装路径；
- 在yarn-env.sh文件修改JDK安装路径；
- 在slaves文件中添加虚拟机名称
- 启动集群：

```shell
/root/app/hadoop/sbin/start-all.sh
```

- jps指令查看进程启动情况

```tex
# node1

2192 NodeManager
1489 DataNode
2503 Jps
2090 ResourceManager
```

```tex
# node2

1888 Jps
1305 DataNode
1769 NodeManager
```

### 1.3 java api操作远端hdfs目录

- 创建maven工程，导入依赖：

```xml
<properties>
    <maven.compiler.source>8</maven.compiler.source>
    <maven.compiler.target>8</maven.compiler.target>
    <hadoop.version>2.10.2</hadoop.version>
</properties>

<dependencies>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-common</artifactId>
        <version>${hadoop.version}</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-hdfs</artifactId>
        <version>${hadoop.version}</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>${hadoop.version}</version>
    </dependency>
    <dependency>
        <groupId>junit</groupId>
        <artifactId>junit</artifactId>
        <version>4.13.2</version>
        <scope>compile</scope>
    </dependency>
</dependencies>
```

- 测试api的前置工作-创建好连接，类似于jdbc的连接：

```java
FileSystem fs = null;

@Before
public void init() throws IOException {
    // 1.构建一个配置参数对象，设置一个参数，即我们需要访问的hdfs的URI
    Configuration conf = new Configuration();
    // 2.这里指定使用的是hdfs文件系统
    conf.set("fs.defaultFS", "hdfs://node1:8020");
    // 3.通过如下的方式进行客户端身份的设置
    System.setProperty("HADOOP_USER_NAME", "root");
    // 4.通过FileSystem的静态方法获取文件系统客户端对象
    fs = FileSystem.get(conf);
}
```

- 测试java c-r-u-d

```java
/**
     * 向hdfs集群添加文件
     * @throws IOException io异常抛出
     */
@Test
public void testAddFileToHdfs() throws IOException {
    // 1.要上传文件的本地目录
    Path src = new Path("D:/test.txt");
    // 2.要上传到hdfs的目标路径
    Path dst = new Path("/data");
    // 3.上传操作
    fs.copyFromLocalFile(src, dst);
    // 4.关闭资源
    fs.close();
}

/**
     * 删除hdfs集群目录或文件
     * @throws IOException io异常
     */
@Test
public void testDeleteFileFromHdfs() throws IOException {
    Path path = new Path("/a");
    fs.delete(path, true);
}

/**
     * 创建目录或重命名文件/目录
     * @throws IOException io异常
     */
@Test
public void testMkdirOrRenameToHdfs() throws IOException {
    fs.mkdirs(new Path("/a/b/c"));
    fs.mkdirs(new Path("/a1/b1/c1"));
    fs.rename(new Path("/a"), new Path("/a3"));
}

/**
     * 获取文件列表
     * @throws IOException io异常
     */
@Test
public void testGetFileList() throws IOException {
    // 1.过去迭代器对象
    RemoteIterator<LocatedFileStatus> listFiles = fs.listFiles(new Path("/"), true);
    while (listFiles.hasNext()) {
        LocatedFileStatus fileStatus = listFiles.next();
        // 文件名
        System.out.println(fileStatus.getPath().getName());
        // 文件块大小
        System.out.println(fileStatus.getBlockSize());
        // 文件权限
        System.out.println(fileStatus.getPermission());
        // 文件内容权限
        System.out.println(fileStatus.getLen());
        // 文件块信息
        BlockLocation[] blockLocations = fileStatus.getBlockLocations();
        for (BlockLocation bl : blockLocations) {
            System.out.println("bl - len: " + bl.getLength()
                               + "--"
                               + "bl - offset" + bl.getOffset());
            String[] hosts = bl.getHosts();
            for (String host : hosts) {
                System.out.println(host);
            }
        }
    }
}
```

## 2.MR/Yarn

### 2.1 java api操作mapreduce-wordcount

- 在本地IDE写好java程序并打包上传至hdfs文件系统的master节点；
- 这里的存放关系为，jar包存放于部署了hadoop集群的节点本地磁盘中，如果为集群环境，需要读入的文件存在在hdfs文件系统中，通过命令执行job：

```shell
hadoop jar <jar包存放路径> <程序中运行类的全限定类名> <读入参数1> ... <读入参数i>
```

- mapreduce本地程序业务逻辑：

```java
package com.mapreduce;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

/**
 * @author LBS59
 * @description mapreduce实现
 */
public class WordCount {
    /**
     * 输入： map阶段的输出 key（Text）一个单词 value（LongWritable） 单词次数 1
     * 输出： key（Text）单词 value（LongWritable） 单词总次数
     */
    public static class WCReducer extends Reducer<Text, LongWritable, Text, LongWritable> {
        private final LongWritable longWritable = new LongWritable(1);

        @Override
        protected void reduce(Text key, Iterable<LongWritable> values,
                              Reducer<Text, LongWritable, Text, LongWritable>.Context context)
                throws IOException, InterruptedException {
            long sum = 0;
            for (LongWritable value : values) {
                sum += value.get();
            }
            longWritable.set(sum);
            // 输出 单词的 和
            context.write(key, longWritable);
        }
    }

    /**
     * 输入： key（LongWritable）一行单词的长度（偏移量） value（Text）一行单词
     * 输出： key（Text）一个单词 value（LongWritable） 单词次数 1
     */
    public static class WCMapper extends Mapper<LongWritable, Text, Text, LongWritable> {
        // 防止对象创建太多
        private final Text text = new Text();
        private final LongWritable longWritable = new LongWritable(1);

        @Override
        protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, LongWritable> Context context)
                throws IOException, InterruptedException {
            // 获取一行
            String line = value.toString();
            // 空格分割 ,一行单词
            String[] words = line.split(" ");
            for (String word : words) {
                text.set(word);
                // 输出到 hadoop key:一个单词 value:次数 1
                context.write(text, longWritable);
            }
        }
    }

    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
        // 获取job
        Job job = Job.getInstance();
        job.setJarByClass(WordCount.class);
        job.setMapperClass(WCMapper.class);
        job.setReducerClass(WCReducer.class);
        // 设置map输出 key 类型
        job.setMapOutputKeyClass(Text.class);
        // 设置map输出 value 类型
        job.setMapOutputValueClass(LongWritable.class);
        // 输入
        FileInputFormat.setInputPaths(job, new Path(args[0]));
        // 输出
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        // 提交job
        boolean ret = job.waitForCompletion(true);
        System.out.println("单词汇总完毕！ ret:" + ret);
    }
}
```

### 2.2 MR性能调优

- 数据输入：

​		在执行MapReduce任务前，将小文件进行合并，大量的小文件会产生大量的map任务，增大map任务装载的次数，而任务的装载比较耗时，从而导致MapReduce运行速度较慢。因此我们采用CombineTextInputFormat来作为输入，解决输入端大量的小文件场景。

- Map阶段：
  - 减少溢写（spill）次数：通过调整io.sort.mb及sort.spill.percent参数值，增大触发spill的内存上限，减少spill次数，从而减少磁盘IO。
  - 减少合并（merge）次数：通过调整io.sort.factor参数，增大merge的文件数目，减少merge的次数，从而缩短mr处理时间。
  - 在map之后，不影响业务逻辑前提下，先进行combine处理，减少 I/O。

> **表1 Map阶段调优属性**

| **属性名称**                              | **类型** | **默认值** | **说明**                                                     |
| ----------------------------------------- | -------- | ---------- | ------------------------------------------------------------ |
| mapreduce.task.io.sort.mb                 | int      | 100        | 配置排序map输出时使用的内存缓冲区的大小，默认100Mb，实际开发中可以设置大一些。 |
| mapreduce.map.sort.spill.percent          | float    | 0.80       | map输出内存缓冲和用来开始磁盘溢出写过程的记录边界索引的阈值，即最大使用环形缓冲内存的阈值。一般默认是80%。也可以直接设置为100% |
| mapreduce.task.io.sort.factor             | int      | 10         | 排序文件时，一次最多合并的流数，实际开发中可将这个值设置为100。 |
| mapreduce.task.min.num.spills.for.combine | int      | 3          | 运行combiner时，所需的最少溢出文件数(如果已指定combiner)     |

- Reduce阶段：
  - 合理设置map和reduce数：两个都不能设置太少，也不能设置太多。太少，会导致task等待，延长处理时间；太多，会导致 map、reduce任务间竞争资源，造成处理超时等错误。
  - 设置map、reduce共存：调整slowstart.completedmaps参数，使map运行到一定程度后，reduce也开始运行，减少reduce的等待时间。
  - 规避使用reduce：因为reduce在用于连接数据集的时候将会产生大量的网络消耗。通过将MapReduce参数setNumReduceTasks设置为0来创建一个只有map的作业。
  - 合理设置reduce端的buffer：默认情况下，数据达到一个阈值的时候，buffer中的数据就会写入磁盘，然后reduce会从磁盘中获得所有的数据。也就是说，buffer和reduce是没有直接关联的，中间多一个写磁盘->读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得buffer中的一部分数据可以直接输送到reduce，从而减少IO开销。这样一来，设置buffer需要内存，读取数据需要内存，reduce计算也要内存，所以要根据作业的运行情况进行调整。

> **表2 Reduce阶段的调优属性**

| **属性名称**                                 | **类型** | **默认值** | **说明**                                                     |
| -------------------------------------------- | -------- | ---------- | ------------------------------------------------------------ |
| mapreduce.job.reduce.slowstart.completedmaps | float    | 0.05       | 当map task在执行到5%，就开始为reduce申请资源。开始执行reduce操作，reduce可以开始拷贝map结果数据和做reduce shuffle操作。 |
| mapred.job.reduce.input.buffer.percent       | float    | 0.0        | 在reduce过程，内存中保存map输出的空间占整个堆空间的比例。如果reducer需要的内存较少，可以增加这个值来最小化访问磁盘的次数。 |

- Shuffle阶段：

​		Shuffle阶段的调优就是给Shuffle过程尽量多地提供内存空间，以防止出现内存溢出现象，可以由参数mapred.child.java.opts来设置，任务节点上的内存大小应尽量大。

> **shuffle阶段的调优属性**

| **属性名称**                  | **类型** | **默认值** | **说明**                                                     |
| ----------------------------- | -------- | ---------- | ------------------------------------------------------------ |
| mapred.map.child.java.opts    |          | -Xmx200m   | 当用户在不设置该值情况下，会以最大1G jvm heap size启动map task，有可能导致内存溢出，所以最简单的做法就是设大参数，一般设置为-Xmx1024m |
| mapred.reduce.child.java.opts |          | -Xmx200m   | 当用户在不设置该值情况下，会以最大1G jvm heap size启动Reduce task，也有可能导致内存溢出，所以最简单的做法就是设大参数，一般设置为-Xmx1024m |

- 其他调优属性：

​		除此之外，MapReduce还有一些基本的资源属性的配置，这些配置的相关参数都位于mapred-default.xml文件中，我们可以合理配置这些属性提高MapReduce性能，表4列举了部分调优属性。

> **表4 MapReduce资源调优属性**

| **属性名称**                            | **类型** | **默认值** | **说明**                                                     |
| --------------------------------------- | -------- | ---------- | ------------------------------------------------------------ |
| mapreduce.map.memory.mb                 | int      | 1024       | 一个Map Task可使用的资源上限。如果Map Task实际使用的资源量超过该值，则会被强制杀死。 |
| mapreduce.reduce.memory.mb              | int      | 1024       | 一个Reduce Task可使用的资源上限。如果Reduce Task实际使用的资源量超过该值，则会被强制杀死。 |
| mapreduce.map.cpu.vcores                | int      | 1          | 每个Map task可使用的最多cpu core数目                         |
| mapreduce.reduce.cpu.vcores             | int      | 1          | 每个Reduce task可使用的最多cpu core数目                      |
| mapreduce.reduce.shuffle.parallelcopies | int      | 5          | 每个reduce去map中拿数据的并行数。                            |
| mapreduce.map.maxattempts               | int      | 4          | 每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败 |
| mapreduce.reduce.maxattempts            | int      | 4          | 每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败 |

## 3.Zookeeper

### 3.1 概述

- 为分布式应用程序提供一个分布式开源协调服务框架，是GoogleChubby的一个开源实现，是Hadoop和Hbase的重要组件，主要用于解决分布式集群中应用系统的一致性问题；

- 提供了类似于Unix系统的目录节点树方式的数据存储；
- 可用于维护和监控存储的数据状态的变化，通过监控来达到数据的集群管理；
- 提供了一组原语(机器命令)，提供java和c语言的接口。

### 3.2 特点

- 也是分布式集群，一个领导者(leader)，多个跟随者(follower)；
- 集群中只要有半数以上的节点存活，Zookeeper集群就能正常服务；
- 全局数据一致性：每个server保存一份相同的数据副本，client无论连接到哪一个server，数据都是一致的；
- 更新请求按顺序进行：来自同一个client的更新请求按其发送顺序依次执行；
- 数据更新的原子性：一次数据的更新要么成功，要么失败；
- 数据的实时性：在一定时间范围内，cliilent能读到最新数据。

### 3.3 应用场景

- 统一配置管理；
- 统一集群管理；
- 服务节点动态上下线感知；
- 软负载均衡；
- 分布式锁；
- 分布式队列。

### 3.4 安装

- 下载解压zookeeper压缩文件；
- 配置环境变量；
- 分发文件到三个节点上；
- 启动半数以上的节点，zookeeper就可以正常对外提供服务。

```xml
jps
# zookeeper启动进程
19556 QuorumPeerMain
```

### 3.5 Shell操作

> **zookeeper的所有shell操作必须声明绝对路径**

| 命令   | 描述                                                         | 样例                                                         |
| ------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| ls     | 查看某个节点下的所有子节点信息                               | ls /                                                         |
| ls2    | 查看某个目录包含的所有文件，与ls不同的是它可以查看一些创建、更新时间，版本信息等 | ls2 /                                                        |
| create | 创建znode，并且需要设置初始内容(写入内容)                    | create /test "test"  --- create -e /test "test"(短暂类型节点) |
| get    | 获取znode的数据                                              | get /test                                                    |
| set    | 修改znode的内容                                              | set /test "test2"                                            |
| delete | 删除znode                                                    | delete /test                                                 |
| quit   | 退出客户端                                                   |                                                              |
| help   | 帮助命令                                                     |                                                              |

## 4.Hive-数据仓库

### 4.1 数据仓库处理

#### 4.1.1 OLTP(在线事务处理)和OLAP(在线分析处理)

- OLTP是传统关系型数据库的主要应用，主要针对的是基本的日常事务处理，例如，银行转账；
- OLAP是数据仓库系统的主要应用，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果，例如，商品的推荐系统。

> **对比：**

| **对比项目** | **OLTP**                       | **OLAP**                       |
| ------------ | ------------------------------ | ------------------------------ |
| **用户**     | 操作人员、底层管理人员         | 决策人员、高级管理人员         |
| **功能**     | 日常操作处理                   | 分析决策                       |
| **DB设计**   | 基于ER模型，面向应用           | 星型/雪花型模型，面向主题      |
| **DB规模**   | GB-TB                          | ≥TB                            |
| **数据**     | 最新的、细节的、二维的、分立的 | 历史的、聚集的、多维的、集成的 |
| **存储规模** | 读/写数条（甚至数百条）记录    | 读上百万条（甚至上亿）记录     |
| **操作频度** | 非常频繁（以秒计）             | 比较稀松（以小时甚至以周计）   |
| **工作单元** | 严格的事务                     | 复杂的查询                     |
| **用户数**   | 数百个-数千万个                | 数个-数百个                    |
| **度量**     | 事务吞吐量                     | 查询吞吐量、响应时间           |

#### 4.1.2 结构

> **四部分：**数据源、数据存储及管理、服务器和前端工具。

![image-20220719103135720](../../../AppData/Roaming/Typora/typora-user-images/image-20220719103135720.png)

#### 4.1.3 数据模型

- 星型模型：在数据仓库建模中，星星模型是维度建模中的一种选择方式。星型模型是以一个事实表和一组维度表组合而成，并且以事实表为中心，所有的维度表直接与事实表相连。
- 雪花型模型：雪花模型的维度表可以拥有其他的维度表，并且维度表与维度表之间是相互关联的。因此，雪花模型相比星型模型更规范一些。但是，由于雪花模型需要关联多层的维度表，因此，性能也比星型模型要低，所以一般不是很常用。

> **事实表和维度表**

- 每个数据仓库都包含一个或者多个事实数据表，事实表是对分析主题的度量，它包含了与各维度表相关联的外键，并通过连接（Join）方式与维度表关联。

  事实表的度量通常是数值类型，且记录数会不断增加，表规模迅速增长。例如现存在一张订单事实表，其字段Prod_id（商品id）可以关联商品维度表、TimeKey（订单时间）可以关联时间维度表等。

- 维度表可以看作用户分析数据的窗口，维度表中包含事实数据表中事实记录的特性，有些特性提供描述性信息，有些特性指定如何汇总事实数据表数据，以便为分析者提供有用的信息。

  维度表包含帮助汇总数据的特性的层次结构，维度是对数据进行分析时特有的一个角度，站在不同角度看待问题，会有不同的结果。例如当分析产品销售情况时，可以选择按照商品类别、商品区域进行分析，此时就构成一个类别、区域的维度。维度表信息较为固定，且数据量小，维度表中的列字段可以将信息分为不同层次的结构级。

### 4.2 Hive

#### 4.2.1 概述

> **Hive是建立在Hadoop文件系统上的数据仓库，它提供了一系列工具，能够对存储在HDFS中的数据进行数据提取、转换和加载（ETL），这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的工具。**

- hive与传统数据库对比：

| **对比项**   | **Hive** | **MySQL**            |
| ------------ | -------- | -------------------- |
| 查询语言     | Hive QL  | SQL                  |
| 数据存储位置 | HDFS     | 块设备、本地文件系统 |
| 数据格式     | 用户定义 | 系统决定             |
| 数据更新     | 不支持   | 支持                 |
| 事务         | 不支持   | 支持                 |
| 执行延迟     | 高       | 低                   |
| 可扩展性     | 高       | 低                   |
| 数据规模     | 大       | 小                   |
| 多表插入     | 支持     | 不支持               |

#### 4.2.2 系统架构

> **Hive是底层封装了Hadoop的数据仓库处理工具，它运行在Hadoop基础上，其系统架构组成主要包含4个部分，分别是用户接口、跨语言服务、底层的驱动引擎以及元数据存储系统。**

![image-20220719104532984](../../../AppData/Roaming/Typora/typora-user-images/image-20220719104532984.png)

- **用户接口：**主要分为三个，分别是CLI、JDBC/ODBC和WebUI。其中，CLI即Shell终端命令行，它是最常用的方式。JDBC/OCBC是Hive的Java实现，与使用传统数据库JDBC的方式类似，WebUI指的是通过浏览器访问Hive。
- **跨语言服务(Thrift Server)：**Thrift是Facebook开发的一个软件框架，可以用来进行可扩展且跨语言的服务。Hive集成了该服务，能让不同的编程语言调用Hive的接口。
- **底层的驱动引擎：**主要包含编译器（Compiler），优化器（Optimizer），执行器（Executor），它们用于完成HQL查询语句从词法分析、语法分析、编译、优化以及查询计划的生成，生成的查询计划存储在HDFS中，并在随后由MapReduce调用执行。
- **元数据存储系统(Metastore)：**Hive中的元数据通常包含表名、列、分区及其相关属性，表数据所在目录的位置信息，Metastore默认存在自带的Derby数据库中。由于Derby数据库不适合多用户操作，并且数据存储目录不固定，不方便管理，因此，通常我们都将元数据存储在MySQL数据库。

#### 4.2.3 工作原理

> **Hive建立在Hadoop之上，那么它和Hadoop之间是如何工作**

![image-20220719105038672](../../../AppData/Roaming/Typora/typora-user-images/image-20220719105038672.png)

- UI将执行的查询操作发送给Driver执行；
- Driver借助查询编译器解析查询，检查语法和查询计划或查询需求；
- 编译器将元数据请求发送到Metastore(任何数据库)；
- 编译器将元数据作为对编译器的响应发送出去；
- 编译器检查需求并将计划重新发送给Driver。至此，查询的解析和编译已经完成；
- Driver将执行计划发送给执行引擎执行Job任务；
- 执行引擎从DataNode上获取结果集，并将结果发送给UI和Driver。

#### 4.2.4 数据模型

> **Hive中所有的数据都存储在HDFS中，它包含数据库（Database）、表（Table）、分区表（Partition）和桶表（Bucket）四种数据类型**

![image-20220719105340842](../../../AppData/Roaming/Typora/typora-user-images/image-20220719105340842.png)

- **数据库：**相当于关系型数据库中的命名空间（namespace），它的作用是将用户和数据库的应用隔离到不同的数据库或者模式中。
- **表：**Hive的表在逻辑上由存储的数据和描述表格数据形式的相关元数据组成。表存储的数据存放在分布式文件系统里，例如HDFS。Hive中的表分为两种类型，一种叫做内部表，这种表的数据存储在Hive数据仓库中，一种叫做外部表，这种表的数据可以存放在Hive数据仓库外的分布式文件系统中，也可以存储在Hive数据仓库中。值得一提的是，Hive数据仓库也就是HDFS中的一个目录，这个目录是Hive数据存储的默认路径，它可以在Hive的配置文件中配置，最终也会存放到元数据库中。
- **分区：**分区：分区的概念是根据“分区列”的值对表的数据进行粗略划分的机制，在Hive存储上的体现就是在表的主目录（Hive的表实际显示就是一个文件夹）下的一个子目录，这个子目录的名字就是我们定义的分区列的名字。
- **桶表：**简单来说，桶表就是把“大表”分成了“小表”。把表或者分区组织成桶表的目的主要是为了获得更高的查询效率，尤其是抽样查询更加便捷。桶表是Hive数据模型的最小单元，数据加载到桶表时，会对字段的值进行哈希取值，然后除以桶个数得到余数进行分桶，保证每个桶中都有数据，在物理上，每个桶表就是表或分区的一个文件。

#### 4.2.5 安装部署

> **内嵌模式：**

- 使用hive自带默认元数据库derby来进行存储，通常用于测试：

  - 优点：
    - 使用简单、不用进行配置；
  - 缺点：
    - 只支持单session。
  - 安装步骤：
    - 1）解压hive并配置环境变量；
    - 2）配置hive-env.sh：如果不存在就用hive-env.sh.template复制一个。

  ```shell
  export JAVA_HOME=/root/app/jdk
  export HADOOP_HOME=/root/app/hadoop
  export HIVE_CONF_DIR=/root/app/hive/conf
  export HIVE_AUX_JARS_PATH=/root/app/hive/lib
  ```

  ​			3）配置hive-site.xml：没有的话将conf/hive-default.xml.template复制一个，

  将hive-site.xml中所有包含${system:java.io.tmpdir}替换为/root/app/hive/iotmp

```shell
%s#${system:java.io.tmpdir}#/root/app/hive/iotmp#g
```

​		将${system:user.name}替换为/root

```shell
%s/${system:user.name}/root/g
```

> **本地模式：**

- 下载安装mysql：

```shell
# mysql密码查看
grep password /var/log/mysqld.log
```

- 修改mysql安全等级并且修改密码允许远程root用户访问

```sql
# 修改mysql安全等级
show variables like '%validate_password%'
set global validate_password_policy=LOW;
set global validate_password_length=4;
set global validate_password_mixed_case_count=0;
set global validate_password_number_count=0;
set global validate_password_char_count=0

# 修改密码
alter user root@localhost identified by '123456';

# 远程授权
grant all privileges on *.* to 'root'@'%' identified by '123456' with grant option;
```

- 配置hive上的mysql部分

```xml
# 使用mysql数据库而不是默认的derby
 <property>
   <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://node3:3306/hive?createDatabaseIfNotExist=true</value>
   <description>
     JDBC connect string for a JDBC metastore.
     To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.
       For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.
    </description>
  </property>

# mysql Driver
 <property>
     <name>javax.jdo.option.ConnectionDriverName</name>
     <value>com.mysql.jdbc.Driver</value>
     <description>Driver class name for a JDBC metastore</description>
</property>

# 连接用户
 <property>
     <name>javax.jdo.option.ConnectionDriverName</name>
     <value>com.mysql.jdbc.Driver</value>
     <description>Driver class name for a JDBC metastore</description>
</property>

# 连接密码
 <property>
     <name>javax.jdo.option.ConnectionDriverName</name>
     <value>com.mysql.jdbc.Driver</value>
     <description>Driver class name for a JDBC metastore</description>
</property>
```

- 下载上传mysql-connector-xxx.jar文件，放在node2中/root/app/hive/lib/
- 执行初始化数据库命---这里使用的jdbc驱动一定要使用-bin.jar结尾的驱动

```shell
schematool --initSchema -dbType mysql
```

- hive命令启动hive，并且不限制路径，任意路径均可

```shell
# 启动hive
hive
```

> **远程模式：**将hive中的相关进程比如hiveserver2或者metastore这样的进程单独开启，使用客户端工具或者命令行进行远程连接这样的服务，即远程模式。

- ==说明：==使用远程模式，需要在hadoop的core-site.xml文件中添加一下属性：

```
<property>
    <name>hadoop.proxyuser.root.hosts</name>
    <value>*</value>
</property>
<property>
    <name>hadoop.proxyuser.root.groups</name>
    <value>*</value>
</property>
```

- 两种连接方式：

  1.1服务端开启hiveserver2进程

```shell
hive --service hiveserver2 &

RunJar进程出现
```

​		1.2客户端连接远程hive

```shell
beeline直接连接

# 无需密码登录
beeline -u jdbc:hive2://node2:10000 -n root
```

​		2.1服务端开启metastore进程

```shell
hive --service metastore &
```

​		2.2客户端连接远程hive

```shell
# 首先在客户端节点配置hive-site.xml
<configuration>
    <property>
        <name>hive.metastore.uris</name>
        <value>thrift://node2:9083</value>
    </property>
</configuration>
```

#### 4.2.6 库操作语法

- 创建数据库；

```sql
create database test1;
create database if not exists test1;
create database if not exists testdb comment 'this is a database of test';
```

- 查看所有数据库；

```sql
show databases;
```

- 切换数据库；

```sql
use test1;
```

- 查看数据库信息；

```sql
desc database test1;
desc database extended test1;
describe database extended test1;
```

- 删除数据库。

```sql
# 只能删除空库
drop database test1;
# 强制删除
drop database testdb cascade;
```

#### 4.2.7 表操作语法

> **数据类型：**

- Hive基本数据类型

| **数据类型**     | **描述**                            |
| ---------------- | ----------------------------------- |
| TINYINT          | 1字节的有符号整数，从-128至127      |
| SMALLINT         | 2字节有符号整数，从-32768至32767    |
| **INT**          | 4字节有符号整数，从从-231到231-1    |
| BIGINT           | 8字节有符号整数，从从-263到263-1    |
| FLOAT            | 4字节单精度浮点数                   |
| DOUBLE           | 8字节双精度浮点数                   |
| DOUBLE PRECISION | Double的别名，从Hive2.2.0开始提供   |
| DECIMAL          | 任意精度的带符号小数                |
| NUMERIC          | 同样是DECIMAL，从Hive3.0开始        |
| TIMESTAMP        | 精度到纳秒的时间戳                  |
| DATE             | 以年/月/日形式描述的日期            |
| INTERVAL         | 表示时间间隔                        |
| **STRING**       | 字符串                              |
| VARCHAR          | 同STRING，字符串长度不固定          |
| CHAR             | 固定长度的字符串                    |
| BOOLEAN          | 用于存储真值（TRUE）和假值（FALSE） |
| BINARY           | 字节数组                            |

- Hive复杂数据类型

| **数据类型** | **描述**                                                     |
| ------------ | ------------------------------------------------------------ |
| ARRAY        | 一组有序字段，字段类型必须相同                               |
| MAP          | 一组无序键值对。键的类型必须是原子类型，值可以是任意类型，同一个映射的键的类型必须相同，值的类型也必须相同 |
| STRUCT       | 一组命名的字段，字段的类型可以不同                           |
| UNION        | 在有限取值范围内的一个值                                     |

- 创建表

```sql
# 创建在当前数据库中
create table t_use(id int,name string);
# 创建在指定数据库中
create table testdb.t_user(id int,name string);
# 指定分割规则形式
create table if not exists emp(
eno int,
ename string,
job string,
mgr int,
hiredate int,
salary int,
comm int,
deptno int
)
row format delimited
fields terminated by ','
lines terminated by '\n'
stored as textfile;
```

- 查看当前表空间中的所有表名

```sql
show tables;

show tables in db;
```

- 查看表结构

```sql
desc tableName;
desc extended tableName;
describe extended tableName;
```

- 修改表结构

```sql
- 修改表名
	alter table oldTableName rename to newTableName

- 修改列名: change column 和修改字段类型是同一个语法
	alter table tableName change column oldName newName colType;
	alter table tableName change column colName colName colType;
	
- 修改列的为之， 2.x版本后，必须是相同类型进行移动位置
	alter table tableName change column colName colName colType after colName1;
	alter table tableName change column colName colName colType first;
	
- 增加字段：add columns
	alter table tableName add columns(sex int, ...)
	
- 删除字段：replace columns # 注意，2.x版本后，注意类型的问题，替换操作，其实涉及到位置的移动问题。
	alter table tableName replace columns(
    	id int,
        name int,
        size int,
        pic string
    );
  注意：实际上是保留小括号内的字段。
```

- 删除表

```sql
drop table tableName;
```

- 数据导入

```sql
create table t_user(
id int,
name string
)
row format delimited
fields terminated by ','
lines terminated by '\n'
stored as textfile;

# 加载数据分两种
- 一种从本地Linux上加载到Hive
- 另外一种是从HDFS加载到Hive中
```

​	1）方法1：使用hdfs dfs -put将本地文件上传到表目录下

```shell
hdfs dfs -put user.txt /user/hive/warehouse/testdb.db/t_user 
```

​	2）方法2：在hive中使用load命令

```shell
load data [local] inpath '文件路径' [overwrite] into table 标明
```

- 从另外一张表(也可以称之为备份表)中动态加载数据

```sql
insert into table tableName2 select [...] from tableName1;
```

- 克隆表数据

```sql
# 复制表结构和数据
create table if not exists tableName2 as select [...] from tableName1;

# 只复制表结构，没有数据
create table if not exists tableName2 like tableName1 location 'tableName1的存储目录的路径'
```

- 数据导出
  - 分类：1）从hive表中导出本地文件系统中(目录、文件)；2）从hive中导出hdfs文件系统中；3）从hive表中导出到其他hive表中。

```sql
- 导出hive表到本地文件系统目录
insert overwrite local directory '本地路径'
row format delimited fields terminated by ','
select * from 'hive表名';
```

```sql
- 直接导入到本地文件系统s的文件中
hive -e 'select * from testdb.t_user' >> 'b'
```

