# 个人学习笔记

## 1.HDFS

### 1.1 架构：

- Master（NameNode/NN）， 带 N 个 Slaves（DataNode/DN）
- 一个存储文件会被拆分成多个Block
  - blocksize：128M（hadoop 2.x版本默认）
  - 130M ==》 2个 Block 128M 和 2M
- NN：
  - 负责客户端请求的相应；
  - 负责存储元数据（文件的名称、副本系数、Block存放的DN）的管理
- DN
  - 存储用户的文件对应的数据块（Block）
  - 定期向NN发送心跳信息，汇报自身以及所有的Block信息，健康状况。
- replication factor：副本系数、副本因子。

### 1.2 搭建Hadoop集群环境(三节点)

> **前置工作：**

- 准备三台虚拟机，这里使用VMWare创建三台Centos7的虚拟机；

| hostname | ip            |
| -------- | ------------- |
| node1    | 192.168.3.100 |
| node2    | 192.168.3.101 |
| node3    | 192.168.3.102 |

- 配置静态ip；
- 关闭防火墙；
- 关闭selinux安全级别，`/etc/selinux/config`中的`SELINUX=disabled`；
- 设置三台虚拟机的域名和IP的配置文件，`vi /etc/hosts`；
- 设置三台虚拟机免密登录-生成密钥以及分发密钥；
- 安装JDK；
- 时间同步：

```shell
#node1虚拟机作为时间主节点，下载ntp

yum install ntp -y

#修改配置文件，vi /etc/ntp.conf

restrict 192.168.254.0 mask 255.255.255.0 nomodify notrap

server 127.127.1.0(自己作为主节点，注释掉server开头的四行)

#启动ntp，并设置为开机启动

systemctl start ntpd

systemctl enable ntpd

#node2和node3作为从节点，下载ntpdate并同步数据

yum install ntpdate -y

ntpdate -u node1

#为了防止时间不一致，做定时器，定时同步node1时间数据,输出信息到空设备文件

crontab -e * * * * * /usr/sbin/ntpdate -u node1 >/dev/null 2>&1
```

- 安装Hadoop。

> **集群搭建：**

- 进到/usr/local/hadoop/etc/hadoop，修改hdfs-site.xml文件，在configuration标签里添加以下内容;

```xml
<!-- core-site.xml -->
<configuration>
    <!-- 在hdfs的地址名称:schame, ip, port -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://node1:8020</value>
    </property>
    <!-- hdfs的一个基础路径，被其他属性所依赖的一个基础路径 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/root/app/hadoop/tmp</value>
    </property>
</configuration>
```

```xml
<!-- hdfs-site.xml -->
<configuration>
    <!-- namenode守护进程管理的元数据文件fsimage存储的位置-->
    <property>
    <name>dfs.namenode.name.dir</name>
    <value>file://${hadoop.tmp.dir}/dfs/name</value>
    </property>
    <!-- 确定DFS数据节点应该将其块存储在本地文件系统的何处-->
    <property>
    <name>dfs.datanode.data.dir</name>
    <value>file://${hadoop.tmp.dir}/dfs/data</value>
    </property>
    <!-- 块的副本数-->
    <property>
    <name>dfs.replication</name>
        <value>3</value>
    </property>
    <!-- 块的大小(128M)，下面的单位是字节-->
    <property>
        <name>dfs.blocksize</name>
        <value>134217728</value>
    </property>
    <!-- secondarynamenode守护进程的http地址:主机名和端口号。参考守护进程布局-->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>node2:50090</value>
    </property>
    <!-- namenode守护进程的http地址:主机名和端口号。参考守护进程布局-->
    <property>
        <name>dfs.namenode.http-address</name>
        <value>node1:50070 </value>
    </property>
</configuration>
```

- 在mapred-site.xml文件configuration标签里添加以下内容-这里不存在mapred-site.xml文件，需要将mapred-site.xml.template文件复制后重命名；

```xml
<!-- mapred-site.xml -->
<configuration>
    <!-- 指定mapreduce使用yarn资源管理器-->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property><!-- 配置作业历史服务器的地址-->
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>node1:10020</value></property>
    <!--配置作业历史服务器的http地址-->
    <property>
        <name>mapreduce.jobhistory.webappaddress</name>
        <value>node1:19888</value>
    </property>
</configuration>
```

- 在yarn-site.xml文件configuration标签里添加以下内容；

```xml
<!-- yarn-site.xml -->
<configuration>
    <!-- 指定yarn的shuffle技术-->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <!-- 指定resourcemanager的主机名-->
    <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>node1</value>
    </property>
    <!--下面的可选-->
    <!--指定shuffle对应的类 -->
    <property>
    <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <!--配置resourcemanager的内部通讯地址-->
    <property>
        <name>yarn.resourcemanager.address</name>
        <value>node1:8032</value>
    </property>
    <!--配置resourcemanager的scheduler的内部通讯地址-->
    <property>
        <name>yarn,resourcemanager.scheduler.address</name>
        <value>node1:8030</value>
    </property>
    <!--配置resoucemanager的资源调度的内部通讯地址-->
    <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>node1:8031</value>
    </property>
    <!--配置resourcemanager的管理员的内部通讯地址-->
    <property>
        <name>yarn.resourcemanager.admin.address</name>
        <value>node1:8033</value>
    </property>
    <!--配置resourcemanager的web ui 的监控页面-->
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>node1:8088</value>
    </property>
</configuration>
```

- 在hadoop-env.sh文件修改JDK安装路径；
- 在yarn-env.sh文件修改JDK安装路径；
- 在slaves文件中添加虚拟机名称
- 启动集群：

```shell
/root/app/hadoop/sbin/start-all.sh
```

- jps指令查看进程启动情况

```tex
# node1

2192 NodeManager
1489 DataNode
2503 Jps
2090 ResourceManager
```

```tex
# node2

1888 Jps
1305 DataNode
1769 NodeManager
```

### 1.3 java api操作远端hdfs目录

- 创建maven工程，导入依赖：

```xml
<properties>
    <maven.compiler.source>8</maven.compiler.source>
    <maven.compiler.target>8</maven.compiler.target>
    <hadoop.version>2.10.2</hadoop.version>
</properties>

<dependencies>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-common</artifactId>
        <version>${hadoop.version}</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-hdfs</artifactId>
        <version>${hadoop.version}</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>${hadoop.version}</version>
    </dependency>
    <dependency>
        <groupId>junit</groupId>
        <artifactId>junit</artifactId>
        <version>4.13.2</version>
        <scope>compile</scope>
    </dependency>
</dependencies>
```

- 测试api的前置工作-创建好连接，类似于jdbc的连接：

```java
FileSystem fs = null;

@Before
public void init() throws IOException {
    // 1.构建一个配置参数对象，设置一个参数，即我们需要访问的hdfs的URI
    Configuration conf = new Configuration();
    // 2.这里指定使用的是hdfs文件系统
    conf.set("fs.defaultFS", "hdfs://node1:8020");
    // 3.通过如下的方式进行客户端身份的设置
    System.setProperty("HADOOP_USER_NAME", "root");
    // 4.通过FileSystem的静态方法获取文件系统客户端对象
    fs = FileSystem.get(conf);
}
```

- 测试java c-r-u-d

```java
/**
     * 向hdfs集群添加文件
     * @throws IOException io异常抛出
     */
@Test
public void testAddFileToHdfs() throws IOException {
    // 1.要上传文件的本地目录
    Path src = new Path("D:/test.txt");
    // 2.要上传到hdfs的目标路径
    Path dst = new Path("/data");
    // 3.上传操作
    fs.copyFromLocalFile(src, dst);
    // 4.关闭资源
    fs.close();
}

/**
     * 删除hdfs集群目录或文件
     * @throws IOException io异常
     */
@Test
public void testDeleteFileFromHdfs() throws IOException {
    Path path = new Path("/a");
    fs.delete(path, true);
}

/**
     * 创建目录或重命名文件/目录
     * @throws IOException io异常
     */
@Test
public void testMkdirOrRenameToHdfs() throws IOException {
    fs.mkdirs(new Path("/a/b/c"));
    fs.mkdirs(new Path("/a1/b1/c1"));
    fs.rename(new Path("/a"), new Path("/a3"));
}

/**
     * 获取文件列表
     * @throws IOException io异常
     */
@Test
public void testGetFileList() throws IOException {
    // 1.过去迭代器对象
    RemoteIterator<LocatedFileStatus> listFiles = fs.listFiles(new Path("/"), true);
    while (listFiles.hasNext()) {
        LocatedFileStatus fileStatus = listFiles.next();
        // 文件名
        System.out.println(fileStatus.getPath().getName());
        // 文件块大小
        System.out.println(fileStatus.getBlockSize());
        // 文件权限
        System.out.println(fileStatus.getPermission());
        // 文件内容权限
        System.out.println(fileStatus.getLen());
        // 文件块信息
        BlockLocation[] blockLocations = fileStatus.getBlockLocations();
        for (BlockLocation bl : blockLocations) {
            System.out.println("bl - len: " + bl.getLength()
                               + "--"
                               + "bl - offset" + bl.getOffset());
            String[] hosts = bl.getHosts();
            for (String host : hosts) {
                System.out.println(host);
            }
        }
    }
}
```

## 2.MR/Yarn

### 2.1 java api操作mapreduce-wordcount

- 在本地IDE写好java程序并打包上传至hdfs文件系统的master节点；
- 这里的存放关系为，jar包存放于部署了hadoop集群的节点本地磁盘中，如果为集群环境，需要读入的文件存在在hdfs文件系统中，通过命令执行job：

```shell
hadoop jar <jar包存放路径> <程序中运行类的全限定类名> <读入参数1> ... <读入参数i>
```

- mapreduce本地程序业务逻辑：

```java
package com.mapreduce;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

/**
 * @author LBS59
 * @description mapreduce实现
 */
public class WordCount {
    /**
     * 输入： map阶段的输出 key（Text）一个单词 value（LongWritable） 单词次数 1
     * 输出： key（Text）单词 value（LongWritable） 单词总次数
     */
    public static class WCReducer extends Reducer<Text, LongWritable, Text, LongWritable> {
        private final LongWritable longWritable = new LongWritable(1);

        @Override
        protected void reduce(Text key, Iterable<LongWritable> values,
                              Reducer<Text, LongWritable, Text, LongWritable>.Context context)
                throws IOException, InterruptedException {
            long sum = 0;
            for (LongWritable value : values) {
                sum += value.get();
            }
            longWritable.set(sum);
            // 输出 单词的 和
            context.write(key, longWritable);
        }
    }

    /**
     * 输入： key（LongWritable）一行单词的长度（偏移量） value（Text）一行单词
     * 输出： key（Text）一个单词 value（LongWritable） 单词次数 1
     */
    public static class WCMapper extends Mapper<LongWritable, Text, Text, LongWritable> {
        // 防止对象创建太多
        private final Text text = new Text();
        private final LongWritable longWritable = new LongWritable(1);

        @Override
        protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, LongWritable> Context context)
                throws IOException, InterruptedException {
            // 获取一行
            String line = value.toString();
            // 空格分割 ,一行单词
            String[] words = line.split(" ");
            for (String word : words) {
                text.set(word);
                // 输出到 hadoop key:一个单词 value:次数 1
                context.write(text, longWritable);
            }
        }
    }

    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
        // 获取job
        Job job = Job.getInstance();
        job.setJarByClass(WordCount.class);
        job.setMapperClass(WCMapper.class);
        job.setReducerClass(WCReducer.class);
        // 设置map输出 key 类型
        job.setMapOutputKeyClass(Text.class);
        // 设置map输出 value 类型
        job.setMapOutputValueClass(LongWritable.class);
        // 输入
        FileInputFormat.setInputPaths(job, new Path(args[0]));
        // 输出
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        // 提交job
        boolean ret = job.waitForCompletion(true);
        System.out.println("单词汇总完毕！ ret:" + ret);
    }
}
```

### 2.2 MR性能调优

- 数据输入：

  ​	在执行MapReduce任务前，将小文件进行合并，大量的小文件会产生大量的map任务，增大map任务装载的次数，而任务的装载比较耗时，从而导致MapReduce运行速度较慢。因此我们采用CombineTextInputFormat来作为输入，解决输入端大量的小文件场景。

- Map阶段：
  - 减少溢写（spill）次数：通过调整io.sort.mb及sort.spill.percent参数值，增大触发spill的内存上限，减少spill次数，从而减少磁盘IO。
  - 减少合并（merge）次数：通过调整io.sort.factor参数，增大merge的文件数目，减少merge的次数，从而缩短mr处理时间。
  - 在map之后，不影响业务逻辑前提下，先进行combine处理，减少 I/O。

> **表1 Map阶段调优属性**

| **属性名称**                                 | **类型** | **默认值** | **说明**                                   |
| ---------------------------------------- | ------ | ------- | ---------------------------------------- |
| mapreduce.task.io.sort.mb                | int    | 100     | 配置排序map输出时使用的内存缓冲区的大小，默认100Mb，实际开发中可以设置大一些。 |
| mapreduce.map.sort.spill.percent         | float  | 0.80    | map输出内存缓冲和用来开始磁盘溢出写过程的记录边界索引的阈值，即最大使用环形缓冲内存的阈值。一般默认是80%。也可以直接设置为100% |
| mapreduce.task.io.sort.factor            | int    | 10      | 排序文件时，一次最多合并的流数，实际开发中可将这个值设置为100。        |
| mapreduce.task.min.num.spills.for.combine | int    | 3       | 运行combiner时，所需的最少溢出文件数(如果已指定combiner)    |

- Reduce阶段：
  - 合理设置map和reduce数：两个都不能设置太少，也不能设置太多。太少，会导致task等待，延长处理时间；太多，会导致 map、reduce任务间竞争资源，造成处理超时等错误。
  - 设置map、reduce共存：调整slowstart.completedmaps参数，使map运行到一定程度后，reduce也开始运行，减少reduce的等待时间。
  - 规避使用reduce：因为reduce在用于连接数据集的时候将会产生大量的网络消耗。通过将MapReduce参数setNumReduceTasks设置为0来创建一个只有map的作业。
  - 合理设置reduce端的buffer：默认情况下，数据达到一个阈值的时候，buffer中的数据就会写入磁盘，然后reduce会从磁盘中获得所有的数据。也就是说，buffer和reduce是没有直接关联的，中间多一个写磁盘->读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得buffer中的一部分数据可以直接输送到reduce，从而减少IO开销。这样一来，设置buffer需要内存，读取数据需要内存，reduce计算也要内存，所以要根据作业的运行情况进行调整。

> **表2 Reduce阶段的调优属性**

| **属性名称**                                 | **类型** | **默认值** | **说明**                                   |
| ---------------------------------------- | ------ | ------- | ---------------------------------------- |
| mapreduce.job.reduce.slowstart.completedmaps | float  | 0.05    | 当map task在执行到5%，就开始为reduce申请资源。开始执行reduce操作，reduce可以开始拷贝map结果数据和做reduce shuffle操作。 |
| mapred.job.reduce.input.buffer.percent   | float  | 0.0     | 在reduce过程，内存中保存map输出的空间占整个堆空间的比例。如果reducer需要的内存较少，可以增加这个值来最小化访问磁盘的次数。 |

- Shuffle阶段：

  ​	Shuffle阶段的调优就是给Shuffle过程尽量多地提供内存空间，以防止出现内存溢出现象，可以由参数mapred.child.java.opts来设置，任务节点上的内存大小应尽量大。

> **shuffle阶段的调优属性**

| **属性名称**                      | **类型** | **默认值**  | **说明**                                   |
| ----------------------------- | ------ | -------- | ---------------------------------------- |
| mapred.map.child.java.opts    |        | -Xmx200m | 当用户在不设置该值情况下，会以最大1G jvm heap size启动map task，有可能导致内存溢出，所以最简单的做法就是设大参数，一般设置为-Xmx1024m |
| mapred.reduce.child.java.opts |        | -Xmx200m | 当用户在不设置该值情况下，会以最大1G jvm heap size启动Reduce task，也有可能导致内存溢出，所以最简单的做法就是设大参数，一般设置为-Xmx1024m |

- 其他调优属性：

  ​	除此之外，MapReduce还有一些基本的资源属性的配置，这些配置的相关参数都位于mapred-default.xml文件中，我们可以合理配置这些属性提高MapReduce性能，表4列举了部分调优属性。

> **表4 MapReduce资源调优属性**

| **属性名称**                                | **类型** | **默认值** | **说明**                                   |
| --------------------------------------- | ------ | ------- | ---------------------------------------- |
| mapreduce.map.memory.mb                 | int    | 1024    | 一个Map Task可使用的资源上限。如果Map Task实际使用的资源量超过该值，则会被强制杀死。 |
| mapreduce.reduce.memory.mb              | int    | 1024    | 一个Reduce Task可使用的资源上限。如果Reduce Task实际使用的资源量超过该值，则会被强制杀死。 |
| mapreduce.map.cpu.vcores                | int    | 1       | 每个Map task可使用的最多cpu core数目               |
| mapreduce.reduce.cpu.vcores             | int    | 1       | 每个Reduce task可使用的最多cpu core数目            |
| mapreduce.reduce.shuffle.parallelcopies | int    | 5       | 每个reduce去map中拿数据的并行数。                    |
| mapreduce.map.maxattempts               | int    | 4       | 每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败 |
| mapreduce.reduce.maxattempts            | int    | 4       | 每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败 |

## 3.Zookeeper

### 3.1 概述

- 为分布式应用程序提供一个分布式开源协调服务框架，是GoogleChubby的一个开源实现，是Hadoop和Hbase的重要组件，主要用于解决分布式集群中应用系统的一致性问题；

- 提供了类似于Unix系统的目录节点树方式的数据存储；
- 可用于维护和监控存储的数据状态的变化，通过监控来达到数据的集群管理；
- 提供了一组原语(机器命令)，提供java和c语言的接口。

### 3.2 特点

- 也是分布式集群，一个领导者(leader)，多个跟随者(follower)；
- 集群中只要有半数以上的节点存活，Zookeeper集群就能正常服务；
- 全局数据一致性：每个server保存一份相同的数据副本，client无论连接到哪一个server，数据都是一致的；
- 更新请求按顺序进行：来自同一个client的更新请求按其发送顺序依次执行；
- 数据更新的原子性：一次数据的更新要么成功，要么失败；
- 数据的实时性：在一定时间范围内，cliilent能读到最新数据。

### 3.3 应用场景

- 统一配置管理；
- 统一集群管理；
- 服务节点动态上下线感知；
- 软负载均衡；
- 分布式锁；
- 分布式队列。

### 3.4 安装

- 下载解压zookeeper压缩文件；
- 配置环境变量；
- 分发文件到三个节点上；
- 启动半数以上的节点，zookeeper就可以正常对外提供服务。

```xml
jps
# zookeeper启动进程
19556 QuorumPeerMain
```

### 3.5 Shell操作

> **zookeeper的所有shell操作必须声明绝对路径**

| 命令     | 描述                                       | 样例                                       |
| ------ | ---------------------------------------- | ---------------------------------------- |
| ls     | 查看某个节点下的所有子节点信息                          | ls /                                     |
| ls2    | 查看某个目录包含的所有文件，与ls不同的是它可以查看一些创建、更新时间，版本信息等 | ls2 /                                    |
| create | 创建znode，并且需要设置初始内容(写入内容)                 | create /test "test"  --- create -e /test "test"(短暂类型节点) |
| get    | 获取znode的数据                               | get /test                                |
| set    | 修改znode的内容                               | set /test "test2"                        |
| delete | 删除znode                                  | delete /test                             |
| quit   | 退出客户端                                    |                                          |
| help   | 帮助命令                                     |                                          |

## 4.Hive-数据仓库

### 4.1 数据仓库处理

#### 4.1.1 OLTP(在线事务处理)和OLAP(在线分析处理)

- OLTP是传统关系型数据库的主要应用，主要针对的是基本的日常事务处理，例如，银行转账；
- OLAP是数据仓库系统的主要应用，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果，例如，商品的推荐系统。

> **对比：**

| **对比项目** | **OLTP**        | **OLAP**        |
| -------- | --------------- | --------------- |
| **用户**   | 操作人员、底层管理人员     | 决策人员、高级管理人员     |
| **功能**   | 日常操作处理          | 分析决策            |
| **DB设计** | 基于ER模型，面向应用     | 星型/雪花型模型，面向主题   |
| **DB规模** | GB-TB           | ≥TB             |
| **数据**   | 最新的、细节的、二维的、分立的 | 历史的、聚集的、多维的、集成的 |
| **存储规模** | 读/写数条（甚至数百条）记录  | 读上百万条（甚至上亿）记录   |
| **操作频度** | 非常频繁（以秒计）       | 比较稀松（以小时甚至以周计）  |
| **工作单元** | 严格的事务           | 复杂的查询           |
| **用户数**  | 数百个-数千万个        | 数个-数百个          |
| **度量**   | 事务吞吐量           | 查询吞吐量、响应时间      |

#### 4.1.2 结构

> **四部分：**数据源、数据存储及管理、服务器和前端工具。

![image-20220719103135720](../../../AppData/Roaming/Typora/typora-user-images/image-20220719103135720.png)

#### 4.1.3 数据模型

- 星型模型：在数据仓库建模中，星星模型是维度建模中的一种选择方式。星型模型是以一个事实表和一组维度表组合而成，并且以事实表为中心，所有的维度表直接与事实表相连。
- 雪花型模型：雪花模型的维度表可以拥有其他的维度表，并且维度表与维度表之间是相互关联的。因此，雪花模型相比星型模型更规范一些。但是，由于雪花模型需要关联多层的维度表，因此，性能也比星型模型要低，所以一般不是很常用。

> **事实表和维度表**

- 每个数据仓库都包含一个或者多个事实数据表，事实表是对分析主题的度量，它包含了与各维度表相关联的外键，并通过连接（Join）方式与维度表关联。

  事实表的度量通常是数值类型，且记录数会不断增加，表规模迅速增长。例如现存在一张订单事实表，其字段Prod_id（商品id）可以关联商品维度表、TimeKey（订单时间）可以关联时间维度表等。

- 维度表可以看作用户分析数据的窗口，维度表中包含事实数据表中事实记录的特性，有些特性提供描述性信息，有些特性指定如何汇总事实数据表数据，以便为分析者提供有用的信息。

  维度表包含帮助汇总数据的特性的层次结构，维度是对数据进行分析时特有的一个角度，站在不同角度看待问题，会有不同的结果。例如当分析产品销售情况时，可以选择按照商品类别、商品区域进行分析，此时就构成一个类别、区域的维度。维度表信息较为固定，且数据量小，维度表中的列字段可以将信息分为不同层次的结构级。

### 4.2 Hive

#### 4.2.1 概述

> **Hive是建立在Hadoop文件系统上的数据仓库，它提供了一系列工具，能够对存储在HDFS中的数据进行数据提取、转换和加载（ETL），这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的工具。**

- hive与传统数据库对比：

| **对比项** | **Hive** | **MySQL**  |
| ------- | -------- | ---------- |
| 查询语言    | Hive QL  | SQL        |
| 数据存储位置  | HDFS     | 块设备、本地文件系统 |
| 数据格式    | 用户定义     | 系统决定       |
| 数据更新    | 不支持      | 支持         |
| 事务      | 不支持      | 支持         |
| 执行延迟    | 高        | 低          |
| 可扩展性    | 高        | 低          |
| 数据规模    | 大        | 小          |
| 多表插入    | 支持       | 不支持        |

#### 4.2.2 系统架构

> **Hive是底层封装了Hadoop的数据仓库处理工具，它运行在Hadoop基础上，其系统架构组成主要包含4个部分，分别是用户接口、跨语言服务、底层的驱动引擎以及元数据存储系统。**

![image-20220719104532984](../../../AppData/Roaming/Typora/typora-user-images/image-20220719104532984.png)

- **用户接口：**主要分为三个，分别是CLI、JDBC/ODBC和WebUI。其中，CLI即Shell终端命令行，它是最常用的方式。JDBC/OCBC是Hive的Java实现，与使用传统数据库JDBC的方式类似，WebUI指的是通过浏览器访问Hive。
- **跨语言服务(Thrift Server)：**Thrift是Facebook开发的一个软件框架，可以用来进行可扩展且跨语言的服务。Hive集成了该服务，能让不同的编程语言调用Hive的接口。
- **底层的驱动引擎：**主要包含编译器（Compiler），优化器（Optimizer），执行器（Executor），它们用于完成HQL查询语句从词法分析、语法分析、编译、优化以及查询计划的生成，生成的查询计划存储在HDFS中，并在随后由MapReduce调用执行。
- **元数据存储系统(Metastore)：**Hive中的元数据通常包含表名、列、分区及其相关属性，表数据所在目录的位置信息，Metastore默认存在自带的Derby数据库中。由于Derby数据库不适合多用户操作，并且数据存储目录不固定，不方便管理，因此，通常我们都将元数据存储在MySQL数据库。

#### 4.2.3 工作原理

> **Hive建立在Hadoop之上，那么它和Hadoop之间是如何工作**

![image-20220719105038672](../../../AppData/Roaming/Typora/typora-user-images/image-20220719105038672.png)

- UI将执行的查询操作发送给Driver执行；
- Driver借助查询编译器解析查询，检查语法和查询计划或查询需求；
- 编译器将元数据请求发送到Metastore(任何数据库)；
- 编译器将元数据作为对编译器的响应发送出去；
- 编译器检查需求并将计划重新发送给Driver。至此，查询的解析和编译已经完成；
- Driver将执行计划发送给执行引擎执行Job任务；
- 执行引擎从DataNode上获取结果集，并将结果发送给UI和Driver。

#### 4.2.4 数据模型

> **Hive中所有的数据都存储在HDFS中，它包含数据库（Database）、表（Table）、分区表（Partition）和桶表（Bucket）四种数据类型**

![image-20220719105340842](../../../AppData/Roaming/Typora/typora-user-images/image-20220719105340842.png)

- **数据库：**相当于关系型数据库中的命名空间（namespace），它的作用是将用户和数据库的应用隔离到不同的数据库或者模式中。
- **表：**Hive的表在逻辑上由存储的数据和描述表格数据形式的相关元数据组成。表存储的数据存放在分布式文件系统里，例如HDFS。Hive中的表分为两种类型，一种叫做内部表，这种表的数据存储在Hive数据仓库中，一种叫做外部表，这种表的数据可以存放在Hive数据仓库外的分布式文件系统中，也可以存储在Hive数据仓库中。值得一提的是，Hive数据仓库也就是HDFS中的一个目录，这个目录是Hive数据存储的默认路径，它可以在Hive的配置文件中配置，最终也会存放到元数据库中。
- **分区：**分区：分区的概念是根据“分区列”的值对表的数据进行粗略划分的机制，在Hive存储上的体现就是在表的主目录（Hive的表实际显示就是一个文件夹）下的一个子目录，这个子目录的名字就是我们定义的分区列的名字。
- **桶表：**简单来说，桶表就是把“大表”分成了“小表”。把表或者分区组织成桶表的目的主要是为了获得更高的查询效率，尤其是抽样查询更加便捷。桶表是Hive数据模型的最小单元，数据加载到桶表时，会对字段的值进行哈希取值，然后除以桶个数得到余数进行分桶，保证每个桶中都有数据，在物理上，每个桶表就是表或分区的一个文件。

#### 4.2.5 安装部署

> **内嵌模式：**

- 使用hive自带默认元数据库derby来进行存储，通常用于测试：

  - 优点：
    - 使用简单、不用进行配置；
  - 缺点：
    - 只支持单session。
  - 安装步骤：
    - 1）解压hive并配置环境变量；
    - 2）配置hive-env.sh：如果不存在就用hive-env.sh.template复制一个。

  ```shell
  export JAVA_HOME=/root/app/jdk
  export HADOOP_HOME=/root/app/hadoop
  export HIVE_CONF_DIR=/root/app/hive/conf
  export HIVE_AUX_JARS_PATH=/root/app/hive/lib
  ```

  ​			3）配置hive-site.xml：没有的话将conf/hive-default.xml.template复制一个，

  将hive-site.xml中所有包含${system:java.io.tmpdir}替换为/root/app/hive/iotmp

```shell
%s#${system:java.io.tmpdir}#/root/app/hive/iotmp#g
```

​		将${system:user.name}替换为/root

```shell
%s/${system:user.name}/root/g
```

> **本地模式：**

- 下载安装mysql：

```shell
# mysql密码查看
grep password /var/log/mysqld.log
```

- 修改mysql安全等级并且修改密码允许远程root用户访问

```sql
# 修改mysql安全等级
show variables like '%validate_password%'
set global validate_password_policy=LOW;
set global validate_password_length=4;
set global validate_password_mixed_case_count=0;
set global validate_password_number_count=0;
set global validate_password_char_count=0

# 修改密码
alter user root@localhost identified by '123456';

# 远程授权
grant all privileges on *.* to 'root'@'%' identified by '123456' with grant option;
```

- 配置hive上的mysql部分

```xml
# 使用mysql数据库而不是默认的derby
 <property>
   <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://node3:3306/hive?createDatabaseIfNotExist=true</value>
   <description>
     JDBC connect string for a JDBC metastore.
     To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.
       For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.
    </description>
  </property>

# mysql Driver
 <property>
     <name>javax.jdo.option.ConnectionDriverName</name>
     <value>com.mysql.jdbc.Driver</value>
     <description>Driver class name for a JDBC metastore</description>
</property>

# 连接用户
 <property>
     <name>javax.jdo.option.ConnectionDriverName</name>
     <value>com.mysql.jdbc.Driver</value>
     <description>Driver class name for a JDBC metastore</description>
</property>

# 连接密码
 <property>
     <name>javax.jdo.option.ConnectionDriverName</name>
     <value>com.mysql.jdbc.Driver</value>
     <description>Driver class name for a JDBC metastore</description>
</property>
```

- 下载上传mysql-connector-xxx.jar文件，放在node2中/root/app/hive/lib/
- 执行初始化数据库命---这里使用的jdbc驱动一定要使用-bin.jar结尾的驱动

```shell
schematool --initSchema -dbType mysql
```

- hive命令启动hive，并且不限制路径，任意路径均可

```shell
# 启动hive
hive
```

> **远程模式：**将hive中的相关进程比如hiveserver2或者metastore这样的进程单独开启，使用客户端工具或者命令行进行远程连接这样的服务，即远程模式。

- ==说明：==使用远程模式，需要在hadoop的core-site.xml文件中添加一下属性：

```
<property>
    <name>hadoop.proxyuser.root.hosts</name>
    <value>*</value>
</property>
<property>
    <name>hadoop.proxyuser.root.groups</name>
    <value>*</value>
</property>
```

- 两种连接方式：

  1.1服务端开启hiveserver2进程

```shell
hive --service hiveserver2 &

RunJar进程出现
```

​		1.2客户端连接远程hive

```shell
beeline直接连接

# 无需密码登录
beeline -u jdbc:hive2://node2:10000 -n root
```

​		2.1服务端开启metastore进程

```shell
hive --service metastore &
```

​		2.2客户端连接远程hive

```shell
# 首先在客户端节点配置hive-site.xml
<configuration>
    <property>
        <name>hive.metastore.uris</name>
        <value>thrift://node2:9083</value>
    </property>
</configuration>
```

#### 4.2.6 库操作语法

- 创建数据库；

```sql
create database test1;
create database if not exists test1;
create database if not exists testdb comment 'this is a database of test';
```

- 查看所有数据库；

```sql
show databases;
```

- 切换数据库；

```sql
use test1;
```

- 查看数据库信息；

```sql
desc database test1;
desc database extended test1;
describe database extended test1;
```

- 删除数据库。

```sql
# 只能删除空库
drop database test1;
# 强制删除
drop database testdb cascade;
```

#### 4.2.7 表操作语法

> **数据类型：**

- Hive基本数据类型

| **数据类型**         | **描述**                   |
| ---------------- | ------------------------ |
| TINYINT          | 1字节的有符号整数，从-128至127      |
| SMALLINT         | 2字节有符号整数，从-32768至32767   |
| **INT**          | 4字节有符号整数，从从-231到231-1    |
| BIGINT           | 8字节有符号整数，从从-263到263-1    |
| FLOAT            | 4字节单精度浮点数                |
| DOUBLE           | 8字节双精度浮点数                |
| DOUBLE PRECISION | Double的别名，从Hive2.2.0开始提供 |
| DECIMAL          | 任意精度的带符号小数               |
| NUMERIC          | 同样是DECIMAL，从Hive3.0开始    |
| TIMESTAMP        | 精度到纳秒的时间戳                |
| DATE             | 以年/月/日形式描述的日期            |
| INTERVAL         | 表示时间间隔                   |
| **STRING**       | 字符串                      |
| VARCHAR          | 同STRING，字符串长度不固定         |
| CHAR             | 固定长度的字符串                 |
| BOOLEAN          | 用于存储真值（TRUE）和假值（FALSE）   |
| BINARY           | 字节数组                     |

- Hive复杂数据类型

| **数据类型** | **描述**                                   |
| -------- | ---------------------------------------- |
| ARRAY    | 一组有序字段，字段类型必须相同                          |
| MAP      | 一组无序键值对。键的类型必须是原子类型，值可以是任意类型，同一个映射的键的类型必须相同，值的类型也必须相同 |
| STRUCT   | 一组命名的字段，字段的类型可以不同                        |
| UNION    | 在有限取值范围内的一个值                             |

- 创建表

```sql
# 创建在当前数据库中
create table t_use(id int,name string);
# 创建在指定数据库中
create table testdb.t_user(id int,name string);
# 指定分割规则形式
create table if not exists emp(
eno int,
ename string,
job string,
mgr int,
hiredate int,
salary int,
comm int,
deptno int
)
row format delimited
fields terminated by ','
lines terminated by '\n'
stored as textfile;
```

- 查看当前表空间中的所有表名

```sql
show tables;

show tables in db;
```

- 查看表结构

```sql
desc tableName;
desc extended tableName;
describe extended tableName;
```

- 修改表结构

```sql
- 修改表名
	alter table oldTableName rename to newTableName

- 修改列名: change column 和修改字段类型是同一个语法
	alter table tableName change column oldName newName colType;
	alter table tableName change column colName colName colType;
	
- 修改列的为之， 2.x版本后，必须是相同类型进行移动位置
	alter table tableName change column colName colName colType after colName1;
	alter table tableName change column colName colName colType first;
	
- 增加字段：add columns
	alter table tableName add columns(sex int, ...)
	
- 删除字段：replace columns # 注意，2.x版本后，注意类型的问题，替换操作，其实涉及到位置的移动问题。
	alter table tableName replace columns(
    	id int,
        name int,
        size int,
        pic string
    );
  注意：实际上是保留小括号内的字段。
```

- 删除表

```sql
drop table tableName;
```

- 数据导入

```sql
create table t_user(
id int,
name string
)
row format delimited
fields terminated by ','
lines terminated by '\n'
stored as textfile;

# 加载数据分两种
- 一种从本地Linux上加载到Hive
- 另外一种是从HDFS加载到Hive中
```

​	1）方法1：使用hdfs dfs -put将本地文件上传到表目录下

```shell
hdfs dfs -put user.txt /user/hive/warehouse/testdb.db/t_user 
```

​	2）方法2：在hive中使用load命令

```shell
load data [local] inpath '文件路径' [overwrite] into table 标明
```

- 从另外一张表(也可以称之为备份表)中动态加载数据

```sql
insert into table tableName2 select [...] from tableName1;
```

- 克隆表数据

```sql
# 复制表结构和数据
create table if not exists tableName2 as select [...] from tableName1;

# 只复制表结构，没有数据
create table if not exists tableName2 like tableName1 location 'tableName1的存储目录的路径'
```

- 数据导出
  - 分类：1）从hive表中导出本地文件系统中(目录、文件)；2）从hive中导出hdfs文件系统中；3）从hive表中导出到其他hive表中。

```sql
- 导出hive表到本地文件系统目录
insert overwrite local directory '本地路径'
row format delimited fields terminated by ','
select * from 'hive表名';
```

```sql
- 直接导入到本地文件系统s的文件中
hive -e 'select * from testdb.t_user' >> '本地路径'
```

#### 4.2.8 表分类

- 内部表

```tex
- 管理表
- 表目录会创建在集群上的{hive.metastore.warehouse.dir}下的相应的库对应的目录中。
- 默认创建的表就是内部表
```

- 外部表

```tex
- 外部表需要使用关键字"external"
- 外部表会根据创建表时的LOCATION指定的路径来创建目录
- 如果没有指定LOCATION，则位置跟内部表相同，一般使用的是第三方提供的或者公用的数据。
- 建表语法:必须指定关键字external
create external table tableName(id int,name string) [location 'localPath']

create external table if not exists t_user7(
id int,
name string
)
row format delimited
fields terminated by ','
location '/publicData';
```

- 内部表和外部表的相互转换

```sql
- 内->外（注意：内部表转外部表，true一定要大写）
alter table tableName set tblproperties('EXTERNAL'='TRUE')

- 外->内(注意：false不区分大小)
alter table tableName set tblproperties('EXTERNAL'='false')
```

#### 4.2.9 Hive Shell小技巧

- 通过shell的参数-e可以执行依次就运行完一个job

```shell
hive -e "hql语句"
# 静默模式：减少冗余日志打印
hive -S -e "hql语句"

# 指定数据库表
hive --database databaseName -e 'hql语句'
```

- 单独执行一个sql文件

```shell
# 通过参数-f+file文件名
hive -f '本地磁盘文件路径'
```

- 在Hive终端执行Linux命令

```shell
# 注意，这里操作的命令都是针对本地文件系统
!+'linux命令'+';结尾'
```

- 执行HDFS命令

```shell
# 可以在Hive的Shell中执行HDFS的DFS命令，不需要敲入前缀hdfs/hadoop
dfs + 'hdfs命令'
```

- 显示当前会话的当前库：

```xml
<!-- 通过配置hive-site.xml让所有session生效 -->
<property>
	<name>hive.cli.print.current.db</name>
    <value>true</value>
    <description>Whether to include the current database in the Hive prompt.</description>
</property>

<!-- 创建.hiverc文件配置生效 -->
set hive.cli.print.current.db = true;
```

- local模式

```sql
- 建议本地调试少量数据时打开本地模式，效率较高
hive.exec.mode.local.auto=true (建议打开)
hive.exec.mode.local.auto.inputbytes.max=134217728
hive.exec.mode.local.auto.input.files.max=4
```

#### 4.2.10 Hive基本查询语法

- 基本使用语句规则

```sql
select ...
from ...
	join [tableName] on ...
	where ...
	group by ...
	having ...
	order by ...
	sort by ...
	limit ...
union | union all ...
```

- 执行顺序

```tex
1.FROM <left_table>
2.ON <join_condition>
3.<join_type> JOIN <right_table>
4.WHERE <where_condition>
5.GROUP BY <group_by_list>
6.HAVING <having_condition>
7.SELECT
8.DISTINCT <select_list>
9.ORDER BY <order_by_condition>
10.LIMIT <limit_number>

- 查询原则
1.尽量不使用子查询、尽量不适用in 或者 not in (可以使用 [not] exists 替代)
2.尽量避免join连接查询
3.查询永远是小表驱动大表(小表作为驱动表)
	-- 注意：内连接，默认左表是驱动表，因此左表一定是小表；
	-- 外连接看需求定
```

- left semi join语法：半开连接。对left join的一种优化形式，只能查询到左表的信息，主要用于解决hive中左表的数据是否存在的问题，相当于exists关键字的使用。

```sql
- left semi join语法
-- 左外连接，左表中的数据全部返回
select * from u1 left join u2 on u1.id = u2.id;
select * from u1 left outer join u2 on u1.id = u2.id;

-- 左半开连接，只显示左表中满足条件的数据，和exists逻辑相同
select * from u1 left semi join u2 on u1.id = u2.id;
-- exists的写法
select * from u1 where exists (select 1 from u2 where u2.id = u1.id);

-- hive中不支持right semi join
```

```sql
- 案例：
emp表：|empno|ename|mgr|job|hiredate|sal|comm|deptno|

- 查询有领导的员工信息
	select * from emp where mgr is not null;
	select * from emp A where exists(select 1 from emp B where B.empno = A.mgr);
	select * from emp A left semi join emp B where A.mgr = B.empno;
	
- 查询有下属的员工信息
	select * from emp A where exists(select 1 from emp B where B.mgr = A.empno);
- 查看有部门的所有员工的信息
	select * from emp A where exists(select 1 from emp B where B.deptno = A.deptno);
```

#### 4.2.11 数据类型

> **基本类型：示例：**

```sql
create table if not exists datatype1(
id1 tinyint,
id2 smallint,
id3 int,
id4 bigint,
salary float,
comm double,
isok boolean,
content binary,
dt timestamp
)
row format delimited
fields terminated by '\t'
;

233	12	342523	455345345	30000	60000	nihao	helloworld	2017-06-02
126	13	342526	455345346	80000	100000	true	helloworld1	2017-06-02	11:41:30

- timestamp 如果是年月日时分秒的格式，必须是写全，才能映射成功。
load data local inpath '本地数据文件目录' into table datatype1;
```

- 自动转换(隐式转换)

```tex
- 在做运算时，小范围类型都可以自动转为大范围类型做运算。
```

> **复杂类型-array：示例**

```sql
create table tableName(
...
colName array<基本类型>
)
说明：下标从0开始，越界不报错，以null代替
```

```sql
zhangsan	78,89,92,96
lisi	67,75,83,94
王五	23,12

create table if not exists arr1(
name string,
scores array<String>
)
row format delimited
fields terminated by '\t'
;
drop table arr2;
create table if not exists arr2(
name string,
scores array<String>
)
row format delimited
fields terminated by '\t'
collection items terminated by ','
;

load data local inpath '/root/doc/user.txt' into table arr1;
load data local inpath '/root/doc/user.txt' into table arr2;
```

```sql
select * from arr1;
select name, scores[1] from arr2;
# 查询总成绩
select nvl(scores[0], 0) + nvl(scores[1], 0) + nvl(scores[2], 0) + nvl(scores[3], 0) from arr2;

# 将array中数据展开 - explode 函数
select name, explode(scores) score from arr2;	- 会报错，这里需要使用到虚拟表

select name, cj from arr2 lateral view explode(scores) mytable as cj;

# 通过虚拟表求所有学生总成绩
select name, sum(cj) from arr2 lateral view explode(scores) mytable as cj group by name;
```

```sql
# 列转行函数操作
- 准备数据
create table arr_tmp
as
select name cj from arr2 lateral view explode(scores) score as cj;

drop table arr3;
create table if not exists arr3(
name string,
scores array<string>
)
row format delimited
fields terminated by ' '
collection items terminated by ','
;

# 使用select_list
```

> **复杂类型-map：示例**

```sql
create table tableName(
...
colName map<T,T>
...
)
```

```sql
zhangsan	chinese:90,math:87,english:63,nature:76
lisi	chinese:60,math:30,english:78,nature:0
wangwu	chinese:89,math:25

create table if not exists map1(
name string,
score map<string,int>
)
row format delimited
fields terminated by '\t'
collection items terminated by ','
map keys terminated by ':'
;

load data local inpath '/root/doc/user.txt' into table map1;
```

```sql
- 练习
# 查询数学大于35分的学生的其他两科成绩
select
m.name,
m.score['english'],
m.score['nature']
from map1 m
where m.score['math'] > 35
;

# 查看每个人的前两科的成绩总和
select
m.name,
m.score['chinese'] + m.score['math']
from map1 m;
```

```sql
- map数据类型的展开和聚合
select explode(score) as (m_obj, m_score) from map1;
# 使用lateral view explode 结合查询
select name, m_obj, m_score from map1 lateral view explode(score) score as m_obj, m_score;
# 对展开成绩求总成绩
select name, sum(m_score) 
from map1 lateral view explode(score) score as
m_obj, m_score group by name;

# 将展开数据存入临时表中
create table map_temp as select name, m_obj, m_score from map1 lateral view explode(score) score as m_obj, m_score;
# str_to_map函数聚合展开数据
select
name,
str_to_map(concat_ws(",", collect_set(concat(m_obj, ":", m_score))), ",", ":")
from map_temp
group by name
;
```

> **复杂类型-struct：示例**

```sql
create table tabName(
...
colName struct<subName1:Type,subName2:Type,...>
...
)
- 调用语法
colName.subName
```

```sql
zhangsan	90,87,63,76
lisi	60,30,78,0
wangwu	89,25,81,9

create table if not exists struct1(
name string,
score struct<chinese:int, math:int, english:int, nature:int>
)
row format delimited
fields terminated by '\t'
collection items terminated by ',';

- 导入数据
load data local inpath '/root/doc/user.txt' into table struct1;
```

```sql
- 练习：查看数学大于35分的学生的英语和语文成绩
select name, score.english, score.chinese from struct1 where score.math > 35;
```

#### 4.2.12 内置函数

- 函数查看

```sql
show functions;
desc function functionName;
```

#### 4.2.13 窗口函数-over函数

```sql
- 案例：user.txt
saml1,2018-01-01,0
saml2,2018-01-02,1
saml3,2018-01-03,2
saml4,2018-01-04,3
saml5,2018-01-05,4
saml6,2018-01-06,5
saml7,2018-01-07,6
saml8,2018-01-08,7
saml9,2018-01-09,8
saml10,2018-01-10,9
saml11,2018-01-11,10
saml12,2018-01-12,11
saml13,2018-01-13,12
saml14,2018-01-14,13

# 创建order表
create table if not exists t_order(
name string,
orderdate string,
cost int
)
row format delimited
fields terminated by ',';

# 加载数据
load data local inpath '/root/doc/user.txt' into table t_order;

# 需求一：查询每一个订单的信息，以及订单的总数；
select * from t_order;
select count(*) from t_order;
- 使用窗口函数，使得开窗内容附加在每一条记录后面，格式为函数+over()函数

# 需求二：查询在2018年1月份购买过的顾客购买明细及总人数
select *, count(*) over()
from t_order
where substring(orderdate, 1, 7) = '2018-01';

-------------------------------------------------
- distrubute by 类似于数据聚合操作
# 需求一：查看顾客购买明细及月购买总额 
select name, orderdate, cost, sum(cost) over(distribute by month(orderdate))
from t_order;

# 需求二：查看顾客的购买明细及每个顾客的月购买总额
select name, orderdate, cost, sum(cost) over(distribute by name, month(orderdate))
from t_order;

---------------------------------------------------
sort by 子句：会让输入的数据强制排序(强调：当使用排序时，窗口会在组内逐行变大)
# 需求一：查看顾客的购买明细及每个顾客的月购买总额，并且按照日期降序排序
select name, orderdate, cost,
sum(cost) over(distribute by name, month(orderdate) sort by orderdate desc)
from t_order;
```

- window子句

> 如果要对窗口的结果做更细粒度的划分，那么就使用window子句，常见的有下面几个

```sql
PRECEDING: 往前
FOLLOWING: 往后
CURRENT ROW: 当前行
UNBOUNDED: 起点
UNBOUNDED PRECEDING: 表示从前面的起点
UNBOUNDED FOLLOWING: 表示到后面的终点

一般window子句都是rows开头
select name, orderdate, cost,
sum(cost) over() as r1,	-- 所有行相加
sum(cost) over(partition by name) as r2,	-- 按name分组，组内数据相加
sum(cost) over(partition by name order by orderdate) as r3,	-- 按name分组，组内数据累加
sum(cost) over(partition by name order by orderdate rows between UNBOUNDED PRECEDING and current row) as r4,	-- 和r3一样，由起点到当前行的聚合
sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING and current row) as r5,	-- 当前行和前面一行及后面一行
sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING and 1 FOLLOWING) as r6,	-- 当前行和前面一行及后面一行
sum(cost) over(partition by name order by orderdate rows between current row and UNBOUNDED FOLLOWING) as r7	-- 当前行那及后面所有行
from t_order;

# 需求：查看顾客到目前为止的购买总额
select name,
t_order.orderdate,
cost,
sum(cost)
over(partition by name order by orderdate rows between UNBOUNDED PRECEDING and current row) as allCount
from t_order;
```

- 序列函数

  - NTILE函数：Hive中很强大的分析函数：它把有序的数据集合平均分配到指定数量个桶中，将桶号分配给每一行。如果不能平均分配，则优先分配较小编号的桶，并且各个桶中能放的行数最多相差1。

  ```sql
  select name, orderdate, cost,
  ntile(3) over(partition by name)
  from t_order;
  ```

  - LAG和LEAD函数
    - lag函数返回当前数据行的前第n行的数据；
    - lead函数返回当前数据行的后第n行的数据；

```sql
# 需求：查询顾客上次购买的时间
select name, orderdate, cost,
lag(orderdate, 1) over(partition by name order by orderdate) as time1
from t_order;

log(colName, n[,default value]):取字段的前第n个值，如果为null，显示默认值

select name, orderdate, cost,
lag(orderdate, 1, '1990-01-01') over(partition by name order by orderdate) as time1
from t_user;
```

FIRST_VALUE和LAST_VALUE函数：按某个字段排序后，到当前行的最小值和最大值所在的行

- 排名函数
  - row_number函数排序，从1开始，按照顺序，生成分组内记录的序列，row_number()的值不会存在重复，按照表中记录的顺序进行排列；
  - RANK()函数，生成数据项在分组中的排名，排名相等会在名次中留下空位；(有重名次会断层)
  - DENSE_RANK()函数，生成数据项在分组中的排名，排名相等不会在名次中留下空位。(有重名次不会断层)

#### 4.2.14 UDF-自定义函数

> hive提供很多的模块可以自定义，比如：自定义函数，serde，输入输出格式等

- UDF:user defined function

```tex
用户自定义函数，一对一的输入输出(最常用).
```

- UDAF:user defined aggregation function

```
用户自定义聚合函数，多对一的输入输出，比如:count sum max
```

- UDTF:user defined table-generate function

```tex
用户自定义表生成函数 一对多的输入输出，比如：lateral view explode
```

1）自定义UDF函数案例

​		准备工作和注意事项

- 在pom.xml加入以下maven的依赖包

```xml
<dependency>
    <groupId>org.apache.hive</groupId>
    <artifactId>hive-exec</artifactId>
    <version>2.1.1</version>
</dependency>
```

- 创建类继承UDF类，编写evaluate()

```java
package com.hive;

import org.apache.hadoop.hive.ql.exec.UDF;

/**
 * @author LBS59
 * @description 自定义UDF函数
 */
public class ConcatString extends UDF {
    public String evaluate(String str) {
        return str + "!";
    }
}
```

- 函数加载方式

  - 第一种：命令加载(只针对当前session有效)

  ```shell
  1.将编写好的程序UDF打包并上传到服务器，将jar包添加到hive的classpath中
  add jar 'classpath'
  2.创建一个自定义的临时函数名字
  create temporary function funcName as 'com.hive.ConcatString';
  3.查看我们创建的自定义函数
  show functions;
  4.在hive中使用函数进行功能测试
  select funcName('a');
  5.如何删除自定义函数？在删除自定义函数的时候需要确定该函数没有被调用
  drop temporary function if exists funcName;
  ```

  - 第二种：启动参数加载(只对当前session有效)

  ```shell
  1.将编写好的自定义函数上传到服务器

  2.写一个配置文件，将添加函数的语句写入配置文件中，hive在启动的时候加载这个配置文件
  vi $HIVE_HOME/conf/hive-init
  文件内容就是方式1中的1~2步

  3.启动hive时
  hive -i $HIVE_HOME/conf/hive-init
  ```

  - 第三种：配置文件加载(只要是hive命令行都是会加载函数)

  ```shell
  1.将编写好的自定义文件上传至服务器
  2.在hive的安装目录下的bin目录中创建一个文件，文件名为.hiverc
  vi ./bin/.hiverc
  3.将添加函数的语句写入这个文件中
  $HIVE_HOME/bin/.hiverc
  add jar 'classpath'
  create temporary function funcName as 'com.hive.ConcatString'
  4.直接启动hive 
  ```

### 4.3  分区表的相关内容

> 如何分区：

​		根据业务需求而定，不过通常以年、月、日】小时、地区等进行分区。

- 分区的语法

```sql
create table tableName(
...
)
partitioned by (colName colType [comment '...'] ...)
```

- 分区的注意事项

```tex
- hive的分区名不区分大小写，不支持中文
- hive的分区字段是一个伪字段，但是可以用来进行操作
- 一张表可以有一个或多个分区，并且分区下面也可以有一个或者多个分区
- 分区以字段的形式在表结构中存在，通过describe tale命令可以查看字段存在，但是该字段不存放实际的数据内容，仅仅是分区的表示。
```

- 创建一级分区语法

```shell
create table if not exists 'tabName'(
id int,
name string
)
partitioned by (dt dtType)
row format delimited
field terminated by '\t'
;

load data local inpath '本地磁盘路径' into table 'tabName' partition('分区描述')
```

- 创建二级分区语法

```shell
create table if not exists 'tabName'(
id int,
name string,
age int
)
partitioned by (year string, month string)
row format delimited
field terminated by '\t'
;

load data local inpath '本地磁盘路径' into table 'tabName' partition('分区描述1', '分区描述2')
```

- 创建三级分区语法

```shell
create table if not exists 'tabName'(
id int,
name string,
age int
)
partitioned by (year string, month string, day string)
row format delimited
field terminated by '\t'
;

load data local inpath '本地磁盘路径' into table 'tabName' partition('分区描述1', '分区描述2', '分区描述3')
```

- 查看分区

```sql
show partitions 'tabName';
desc 'tabName';
```

- 修改分区位置

```sql
alter table 'tabName' partition('分区描述。。。') set location 'hdfs://node1:8020 + 绝对路径'
```

- 增加分区

```sql
# 不带数据
alter table 'tabName' add partition(分区描述。。。) 。。。;

# 带数据
alter table 'tabName' add partition(分区描述。。。) 。。。 location 'hdfs绝对路径';

# 添加多个分区
alter table 'tabName' add
partiiton (...)
partition (...)
...
partition (...);
```

- 删除分区

```sql
alter table 'tabName' drop
partition (...)
partition (...)
...
partition (...);
```

> 分区类型详解

```tex
1.静态分区：直接加载文件到指定的分区，即静态分区表；
2.动态分区：数据未知，根据分区的值来确定需要创建的分区(分区目录不是指定的，而是根据数据的值自动分配的)
3.混合分区：静态和动态都有。
```

- 创建动态分区流程

```sql
1.创建动态分区表
create table 'tbName'(
id int,
name string,
gender string,
age int
academy string
)
partitioned by ('分区描述')
row format delimited
fields terminated by ','
;

2.动态分区加载数据
- 先创建临时表
create table 'temp_tabName'(
id int,
name string,
gender string,
age int,
academy string,
'分区字段')
row format delimited
fields terminated by ','
;

3.本地创建数据并导入临时表

4.动态加载数据到动态分区表
insert into 'tabName' partition(分区字段) select id, name, gender, age, academy, '分区字段' from 'tmp_tabName';
- 设置分区属性
hive.exec.dynamic.partition=true	# 是否支持动态分区操作
hive.exec.dynamic.partition.mode=strict/nonstrict	# 严格模式/非严格模式
hive.exec.max.dynamic.partitions=100	# 总共允许创建的动态分区的最大数量
hive.exec.max.dynamic.partitions.pernode=100	#  每一个节点的mr数量
```

### 4.4 Hive索引与视图

#### 4.4.1 索引

- 创建表并加载数据
- 创建索引

```sql
create index '索引名'
on table '表名(字段名)'
as '索引文件的存储格式'
with deferred rebuild	-- 索引能够重建
;
```

- 修改索引(重建索引)：目的产生索引文件

```sql
alter index '索引名'
on '表名' rebuild;
```

- 查看索引

```sql
show index on '表名';
```

- 验证性能

```sql
select count(*) from '表名';

select * from '表名' where '索引条件'
```

- 创建联合索引

```sql
create index '索引名'
on table '表名(多个字段名)'
as '索引文件的存储格式'
with deferred rebuild;
```

- 删除索引

```sql
drop index '索引名' on '表名';
```

#### 4.4.2 视图

- 创建视图

```sql
create view if not exists '视图名' as select '查询字段' from '表名' where '筛选条件';
```

- 查看视图

```sql
show tables;
# 查看视图创建的方式
show create table '视图名';
desc '视图名';
```

- 视图是否可以克隆

```tex
- 没必要对视图进行克隆，因为视图没有数据存储；
- 修改视图：直接修改元数据(修改元数据中查询)；
- 先删除再创建就可以了
```

- 删除视图

```sql
drop view if exists '索引名';
注意：
1.切忌先删除视图所依赖的表再去查询视图；
2.视图不能用insert into 或者 load加载数据；
3.视图是只读的不能修改其结构、相关属性。
```

### 4.5 Hive的压缩

#### 4.5.1 hive在map阶段的压缩

> map阶段的设置，就是在MapReduce的shuffle阶段对mapper产生的中间结果数据压缩。在这个阶段优先选择一个低cpu开销的算法

```xml
<!-- 指定要不要开启中间压缩 -->
<property>
    <name>hive.exec.compress.intermediate</name>
    <value>false</value>
</property>
<!-- 指定中间压缩想要使用的压缩编码器 -->
<property>
    <name>hive.intermediate.compress.codec</name>
    <value/>
</property>
<!-- 指定压缩编码器中的压缩类型 -->
<property>
    <name>hive.intermediate.compress.type</name>
    <value/>
</property>
```

#### 4.5.2 hive在reduce阶段的压缩

> 即对reduce阶段的输出数据进行压缩设置

```xml
<!-- 指定要不要开启最终压缩 -->
<property>
    <name>hive.exec.compress.output</name>
    <value>false</value>
</property>

-- 注意：如果开启，默认使用中间压缩配置的压缩编码器和压缩类型。
```

#### 4.5.3 常用压缩格式

| 压缩格式   | 压缩比  | 压缩速度 | 需要安装 | 支持切分      |
| ------ | ---- | ---- | ---- | --------- |
| bzip2  | 最高   | 慢    | 否    | 是         |
| gzip   | 很高   | 比较快  | 否    | 否         |
| snappy | 比较高  | 很快   | 是    | 否         |
| lzo    | 比较高  | 很快   | 是    | 是(需要建立索引) |

#### 4.5.4 压缩编码器

| 压缩格式    | 压缩编码器                                    |
| ------- | ---------------------------------------- |
| deflate | org.apache.hadoop.io.compress.DefaultCodec |
| gzip    | org.apache.hadoop.io.compress.GzipCodec  |
| bzip2   | org.apache.hadoop.io.compress.BZip2Codec |
| lzo     | com.hadoop.compression.lzo.LzopCodec（中间输出使用） |
| snappy  | org.apache.hadoop.io.compress.SnappyCodec（中间输出使用） |

## 5.Spark

### 5.1 Spark概述

#### 5.1.1 Spark运行模式

- 本地模式(单机)：本地模式就是一个独立的进程，通过其内部的多个线程来模拟整个Spark运行时环境
- Standalone模式(集群)：Spark中的各个角色以独立进程的形式存在，并组成Spark集群环境
- Hadoop YARN模式(集群)：Spark中的各个角色运行在YARN的容器内部，并组成Spark集群环境
- Kubernetes模拟(容器集群)：Spark中的各个角色运行在Kubernetes的容器内部，并组成Spark集群环境
- 。。。云服务模式

#### 5.1.2 Spark的架构角色

- 回顾之前YARN角色：
  - 资源管理层面
    - 集群资源管理者(Master)：ResourceManager；
    - 单机资源管理者(Worker)：NodeManager。
  - 任务计算层面
    - 单任务管理者(Master)：ApplicationMaster；
    - 单任务执行者(Worker)：Task(容器内计算框架的主要角色)。
- Spark角色：
  - 资源管理层面
    - 集群资源管理者：Master；
    - 单机资源管理者：Worker。
  - 任务计算层面
    - 单任务管理者：Driver；(在Local模式下，只有Driver，即是任务管理者又是任务执行者)
    - 单任务执行者：Executor。

#### 5.1.3 总结

> Spark解决什么问题？

- 海量数据的计算，可以进行离线批处理以及实时流计算；

> Spark有哪些模块？

- 核心SparkCore、SQL计算(SparkSQL)、流计算(SparkStreaming)、图计算(GraphX)、机器学习(MLlib)

> Spark有哪些特点？

- 速度快、使用简单、通用性强、多种模式运行。

> Spark的运行模式？

- 本地模式；
- 集群模式(StandAlone、YARN、K8S)
- 云模式

> Spark的运行角色(对比YARN)？

- Master：集群资源管理；
- Worker：单机资源管理；
- Driver：单任务管理者；
- Executor：单任务执行者。