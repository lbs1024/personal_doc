# 个人学习笔记

## 1.HDFS

### 1.1 架构：

- Master（NameNode/NN）， 带 N 个 Slaves（DataNode/DN）
- 一个存储文件会被拆分成多个Block
  - blocksize：128M（hadoop 2.x版本默认）
  - 130M ==》 2个 Block 128M 和 2M
- NN：
  - 负责客户端请求的相应；
  - 负责存储元数据（文件的名称、副本系数、Block存放的DN）的管理
- DN
  - 存储用户的文件对应的数据块（Block）
  - 定期向NN发送心跳信息，汇报自身以及所有的Block信息，健康状况。
- replication factor：副本系数、副本因子。

### 1.2 搭建Hadoop集群环境(三节点)

> **前置工作：**

- 准备三台虚拟机，这里使用VMWare创建三台Centos7的虚拟机；

| hostname | ip            |
| -------- | ------------- |
| node1    | 192.168.3.100 |
| node2    | 192.168.3.101 |
| node3    | 192.168.3.102 |

- 配置静态ip；
- 关闭防火墙；
- 关闭selinux安全级别，`/etc/selinux/config`中的`SELINUX=disabled`；
- 设置三台虚拟机的域名和IP的配置文件，`vi /etc/hosts`；
- 设置三台虚拟机免密登录-生成密钥以及分发密钥；
- 安装JDK；
- 时间同步：

```shell
#node1虚拟机作为时间主节点，下载ntp

yum install ntp -y

#修改配置文件，vi /etc/ntp.conf

restrict 192.168.254.0 mask 255.255.255.0 nomodify notrap

server 127.127.1.0(自己作为主节点，注释掉server开头的四行)

#启动ntp，并设置为开机启动

systemctl start ntpd

systemctl enable ntpd

#node2和node3作为从节点，下载ntpdate并同步数据

yum install ntpdate -y

ntpdate -u node1

#为了防止时间不一致，做定时器，定时同步node1时间数据,输出信息到空设备文件

crontab -e * * * * * /usr/sbin/ntpdate -u node1 >/dev/null 2>&1
```

- 安装Hadoop。

> **集群搭建：**

- 进到/usr/local/hadoop/etc/hadoop，修改hdfs-site.xml文件，在configuration标签里添加以下内容;

```xml
<!-- core-site.xml -->
<configuration>
    <!-- 在hdfs的地址名称:schame, ip, port -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://node1:8020</value>
    </property>
    <!-- hdfs的一个基础路径，被其他属性所依赖的一个基础路径 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/root/app/hadoop/tmp</value>
    </property>
</configuration>
```

```xml
<!-- hdfs-site.xml -->
<configuration>
    <!-- namenode守护进程管理的元数据文件fsimage存储的位置-->
    <property>
    <name>dfs.namenode.name.dir</name>
    <value>file://${hadoop.tmp.dir}/dfs/name</value>
    </property>
    <!-- 确定DFS数据节点应该将其块存储在本地文件系统的何处-->
    <property>
    <name>dfs.datanode.data.dir</name>
    <value>file://${hadoop.tmp.dir}/dfs/data</value>
    </property>
    <!-- 块的副本数-->
    <property>
    <name>dfs.replication</name>
        <value>3</value>
    </property>
    <!-- 块的大小(128M)，下面的单位是字节-->
    <property>
        <name>dfs.blocksize</name>
        <value>134217728</value>
    </property>
    <!-- secondarynamenode守护进程的http地址:主机名和端口号。参考守护进程布局-->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>node2:50090</value>
    </property>
    <!-- namenode守护进程的http地址:主机名和端口号。参考守护进程布局-->
    <property>
        <name>dfs.namenode.http-address</name>
        <value>node1:50070 </value>
    </property>
</configuration>
```

- 在mapred-site.xml文件configuration标签里添加以下内容-这里不存在mapred-site.xml文件，需要将mapred-site.xml.template文件复制后重命名；

```xml
<!-- mapred-site.xml -->
<configuration>
    <!-- 指定mapreduce使用yarn资源管理器-->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property><!-- 配置作业历史服务器的地址-->
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>node1:10020</value></property>
    <!--配置作业历史服务器的http地址-->
    <property>
        <name>mapreduce.jobhistory.webappaddress</name>
        <value>node1:19888</value>
    </property>
</configuration>
```

- 在yarn-site.xml文件configuration标签里添加以下内容；

```xml
<!-- yarn-site.xml -->
<configuration>
    <!-- 指定yarn的shuffle技术-->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <!-- 指定resourcemanager的主机名-->
    <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>node1</value>
    </property>
    <!--下面的可选-->
    <!--指定shuffle对应的类 -->
    <property>
    <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <!--配置resourcemanager的内部通讯地址-->
    <property>
        <name>yarn.resourcemanager.address</name>
        <value>node1:8032</value>
    </property>
    <!--配置resourcemanager的scheduler的内部通讯地址-->
    <property>
        <name>yarn,resourcemanager.scheduler.address</name>
        <value>node1:8030</value>
    </property>
    <!--配置resoucemanager的资源调度的内部通讯地址-->
    <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>node1:8031</value>
    </property>
    <!--配置resourcemanager的管理员的内部通讯地址-->
    <property>
        <name>yarn.resourcemanager.admin.address</name>
        <value>node1:8033</value>
    </property>
    <!--配置resourcemanager的web ui 的监控页面-->
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>node1:8088</value>
    </property>
</configuration>
```

- 在hadoop-env.sh文件修改JDK安装路径；
- 在yarn-env.sh文件修改JDK安装路径；
- 在slaves文件中添加虚拟机名称
- 启动集群：

```shell
/root/app/hadoop/sbin/start-all.sh
```

- jps指令查看进程启动情况

```tex
# node1

2192 NodeManager
1489 DataNode
2503 Jps
2090 ResourceManager
```

```tex
# node2

1888 Jps
1305 DataNode
1769 NodeManager
```

### 1.3 java api操作远端hdfs目录

- 创建maven工程，导入依赖：

```xml
<properties>
    <maven.compiler.source>8</maven.compiler.source>
    <maven.compiler.target>8</maven.compiler.target>
    <hadoop.version>2.10.2</hadoop.version>
</properties>

<dependencies>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-common</artifactId>
        <version>${hadoop.version}</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-hdfs</artifactId>
        <version>${hadoop.version}</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>${hadoop.version}</version>
    </dependency>
    <dependency>
        <groupId>junit</groupId>
        <artifactId>junit</artifactId>
        <version>4.13.2</version>
        <scope>compile</scope>
    </dependency>
</dependencies>
```

- 测试api的前置工作-创建好连接，类似于jdbc的连接：

```java
FileSystem fs = null;

@Before
public void init() throws IOException {
    // 1.构建一个配置参数对象，设置一个参数，即我们需要访问的hdfs的URI
    Configuration conf = new Configuration();
    // 2.这里指定使用的是hdfs文件系统
    conf.set("fs.defaultFS", "hdfs://node1:8020");
    // 3.通过如下的方式进行客户端身份的设置
    System.setProperty("HADOOP_USER_NAME", "root");
    // 4.通过FileSystem的静态方法获取文件系统客户端对象
    fs = FileSystem.get(conf);
}
```

- 测试java c-r-u-d

```java
/**
     * 向hdfs集群添加文件
     * @throws IOException io异常抛出
     */
@Test
public void testAddFileToHdfs() throws IOException {
    // 1.要上传文件的本地目录
    Path src = new Path("D:/test.txt");
    // 2.要上传到hdfs的目标路径
    Path dst = new Path("/data");
    // 3.上传操作
    fs.copyFromLocalFile(src, dst);
    // 4.关闭资源
    fs.close();
}

/**
     * 删除hdfs集群目录或文件
     * @throws IOException io异常
     */
@Test
public void testDeleteFileFromHdfs() throws IOException {
    Path path = new Path("/a");
    fs.delete(path, true);
}

/**
     * 创建目录或重命名文件/目录
     * @throws IOException io异常
     */
@Test
public void testMkdirOrRenameToHdfs() throws IOException {
    fs.mkdirs(new Path("/a/b/c"));
    fs.mkdirs(new Path("/a1/b1/c1"));
    fs.rename(new Path("/a"), new Path("/a3"));
}

/**
     * 获取文件列表
     * @throws IOException io异常
     */
@Test
public void testGetFileList() throws IOException {
    // 1.过去迭代器对象
    RemoteIterator<LocatedFileStatus> listFiles = fs.listFiles(new Path("/"), true);
    while (listFiles.hasNext()) {
        LocatedFileStatus fileStatus = listFiles.next();
        // 文件名
        System.out.println(fileStatus.getPath().getName());
        // 文件块大小
        System.out.println(fileStatus.getBlockSize());
        // 文件权限
        System.out.println(fileStatus.getPermission());
        // 文件内容权限
        System.out.println(fileStatus.getLen());
        // 文件块信息
        BlockLocation[] blockLocations = fileStatus.getBlockLocations();
        for (BlockLocation bl : blockLocations) {
            System.out.println("bl - len: " + bl.getLength()
                               + "--"
                               + "bl - offset" + bl.getOffset());
            String[] hosts = bl.getHosts();
            for (String host : hosts) {
                System.out.println(host);
            }
        }
    }
}
```

## 2.MR

### 2.1 java api操作mapreduce-wordcount

- 在本地IDE写好java程序并打包上传至hdfs文件系统的master节点；
- 这里的存放关系为，jar包存放于部署了hadoop集群的节点本地磁盘中，如果为集群环境，需要读入的文件存在在hdfs文件系统中，通过命令执行job：

```shell
hadoop jar <jar包存放路径> <程序中运行类的全限定类名> <读入参数1> ... <读入参数i>
```

- mapreduce本地程序业务逻辑：

```java
package com.mapreduce;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

/**
 * @author LBS59
 * @description mapreduce实现
 */
public class WordCount {
    /**
     * 输入： map阶段的输出 key（Text）一个单词 value（LongWritable） 单词次数 1
     * 输出： key（Text）单词 value（LongWritable） 单词总次数
     */
    public static class WCReducer extends Reducer<Text, LongWritable, Text, LongWritable> {
        private final LongWritable longWritable = new LongWritable(1);

        @Override
        protected void reduce(Text key, Iterable<LongWritable> values,
                              Reducer<Text, LongWritable, Text, LongWritable>.Context context)
                throws IOException, InterruptedException {
            long sum = 0;
            for (LongWritable value : values) {
                sum += value.get();
            }
            longWritable.set(sum);
            // 输出 单词的 和
            context.write(key, longWritable);
        }
    }

    /**
     * 输入： key（LongWritable）一行单词的长度（偏移量） value（Text）一行单词
     * 输出： key（Text）一个单词 value（LongWritable） 单词次数 1
     */
    public static class WCMapper extends Mapper<LongWritable, Text, Text, LongWritable> {
        // 防止对象创建太多
        private final Text text = new Text();
        private final LongWritable longWritable = new LongWritable(1);

        @Override
        protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, LongWritable> Context context)
                throws IOException, InterruptedException {
            // 获取一行
            String line = value.toString();
            // 空格分割 ,一行单词
            String[] words = line.split(" ");
            for (String word : words) {
                text.set(word);
                // 输出到 hadoop key:一个单词 value:次数 1
                context.write(text, longWritable);
            }
        }
    }

    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
        // 获取job
        Job job = Job.getInstance();
        job.setJarByClass(WordCount.class);
        job.setMapperClass(WCMapper.class);
        job.setReducerClass(WCReducer.class);
        // 设置map输出 key 类型
        job.setMapOutputKeyClass(Text.class);
        // 设置map输出 value 类型
        job.setMapOutputValueClass(LongWritable.class);
        // 输入
        FileInputFormat.setInputPaths(job, new Path(args[0]));
        // 输出
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        // 提交job
        boolean ret = job.waitForCompletion(true);
        System.out.println("单词汇总完毕！ ret:" + ret);
    }
}
```

### 2.2 MR性能调优

- 数据输入：

​		在执行MapReduce任务前，将小文件进行合并，大量的小文件会产生大量的map任务，增大map任务装载的次数，而任务的装载比较耗时，从而导致MapReduce运行速度较慢。因此我们采用CombineTextInputFormat来作为输入，解决输入端大量的小文件场景。

- Map阶段：
  - 减少溢写（spill）次数：通过调整io.sort.mb及sort.spill.percent参数值，增大触发spill的内存上限，减少spill次数，从而减少磁盘IO。
  - 减少合并（merge）次数：通过调整io.sort.factor参数，增大merge的文件数目，减少merge的次数，从而缩短mr处理时间。
  - 在map之后，不影响业务逻辑前提下，先进行combine处理，减少 I/O。

> **表1 Map阶段调优属性**

| **属性名称**                              | **类型** | **默认值** | **说明**                                                     |
| ----------------------------------------- | -------- | ---------- | ------------------------------------------------------------ |
| mapreduce.task.io.sort.mb                 | int      | 100        | 配置排序map输出时使用的内存缓冲区的大小，默认100Mb，实际开发中可以设置大一些。 |
| mapreduce.map.sort.spill.percent          | float    | 0.80       | map输出内存缓冲和用来开始磁盘溢出写过程的记录边界索引的阈值，即最大使用环形缓冲内存的阈值。一般默认是80%。也可以直接设置为100% |
| mapreduce.task.io.sort.factor             | int      | 10         | 排序文件时，一次最多合并的流数，实际开发中可将这个值设置为100。 |
| mapreduce.task.min.num.spills.for.combine | int      | 3          | 运行combiner时，所需的最少溢出文件数(如果已指定combiner)     |

- Reduce阶段：
  - 合理设置map和reduce数：两个都不能设置太少，也不能设置太多。太少，会导致task等待，延长处理时间；太多，会导致 map、reduce任务间竞争资源，造成处理超时等错误。
  - 设置map、reduce共存：调整slowstart.completedmaps参数，使map运行到一定程度后，reduce也开始运行，减少reduce的等待时间。
  - 规避使用reduce：因为reduce在用于连接数据集的时候将会产生大量的网络消耗。通过将MapReduce参数setNumReduceTasks设置为0来创建一个只有map的作业。
  - 合理设置reduce端的buffer：默认情况下，数据达到一个阈值的时候，buffer中的数据就会写入磁盘，然后reduce会从磁盘中获得所有的数据。也就是说，buffer和reduce是没有直接关联的，中间多一个写磁盘->读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得buffer中的一部分数据可以直接输送到reduce，从而减少IO开销。这样一来，设置buffer需要内存，读取数据需要内存，reduce计算也要内存，所以要根据作业的运行情况进行调整。

> **表2 Reduce阶段的调优属性**

| **属性名称**                                 | **类型** | **默认值** | **说明**                                                     |
| -------------------------------------------- | -------- | ---------- | ------------------------------------------------------------ |
| mapreduce.job.reduce.slowstart.completedmaps | float    | 0.05       | 当map task在执行到5%，就开始为reduce申请资源。开始执行reduce操作，reduce可以开始拷贝map结果数据和做reduce shuffle操作。 |
| mapred.job.reduce.input.buffer.percent       | float    | 0.0        | 在reduce过程，内存中保存map输出的空间占整个堆空间的比例。如果reducer需要的内存较少，可以增加这个值来最小化访问磁盘的次数。 |

- Shuffle阶段：

​		Shuffle阶段的调优就是给Shuffle过程尽量多地提供内存空间，以防止出现内存溢出现象，可以由参数mapred.child.java.opts来设置，任务节点上的内存大小应尽量大。

> **shuffle阶段的调优属性**

| **属性名称**                  | **类型** | **默认值** | **说明**                                                     |
| ----------------------------- | -------- | ---------- | ------------------------------------------------------------ |
| mapred.map.child.java.opts    |          | -Xmx200m   | 当用户在不设置该值情况下，会以最大1G jvm heap size启动map task，有可能导致内存溢出，所以最简单的做法就是设大参数，一般设置为-Xmx1024m |
| mapred.reduce.child.java.opts |          | -Xmx200m   | 当用户在不设置该值情况下，会以最大1G jvm heap size启动Reduce task，也有可能导致内存溢出，所以最简单的做法就是设大参数，一般设置为-Xmx1024m |

- 其他调优属性：

​		除此之外，MapReduce还有一些基本的资源属性的配置，这些配置的相关参数都位于mapred-default.xml文件中，我们可以合理配置这些属性提高MapReduce性能，表4列举了部分调优属性。

> **表4 MapReduce资源调优属性**

| **属性名称**                            | **类型** | **默认值** | **说明**                                                     |
| --------------------------------------- | -------- | ---------- | ------------------------------------------------------------ |
| mapreduce.map.memory.mb                 | int      | 1024       | 一个Map Task可使用的资源上限。如果Map Task实际使用的资源量超过该值，则会被强制杀死。 |
| mapreduce.reduce.memory.mb              | int      | 1024       | 一个Reduce Task可使用的资源上限。如果Reduce Task实际使用的资源量超过该值，则会被强制杀死。 |
| mapreduce.map.cpu.vcores                | int      | 1          | 每个Map task可使用的最多cpu core数目                         |
| mapreduce.reduce.cpu.vcores             | int      | 1          | 每个Reduce task可使用的最多cpu core数目                      |
| mapreduce.reduce.shuffle.parallelcopies | int      | 5          | 每个reduce去map中拿数据的并行数。                            |
| mapreduce.map.maxattempts               | int      | 4          | 每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败 |
| mapreduce.reduce.maxattempts            | int      | 4          | 每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败 |
